{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12370997",
   "metadata": {},
   "source": [
    "# 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece6f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "Successfully installed pip-25.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting llmcompressor\n",
      "  Downloading llmcompressor-0.9.0.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: loguru<=0.7.3,>=0.7.2 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (0.7.3)\n",
      "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (6.0.2)\n",
      "Requirement already satisfied: numpy<=2.3.5,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.2.5)\n",
      "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.32.3)\n",
      "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (4.67.1)\n",
      "Requirement already satisfied: torch<=2.9.1,>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.9.1)\n",
      "Collecting transformers<=4.57.3,>=4.54.0 (from llmcompressor)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets<=4.4.1,>=4.0.0 (from llmcompressor)\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate<=1.12.0,>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (1.12.0)\n",
      "Collecting nvidia-ml-py<=13.590.44,>=12.560.30 (from llmcompressor)\n",
      "  Downloading nvidia_ml_py-13.590.44-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: pillow<=12.0.0,>=10.4.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (11.0.0)\n",
      "Requirement already satisfied: compressed-tensors==0.13.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (0.13.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.11/site-packages (from compressed-tensors==0.13.0->llmcompressor) (2.12.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (7.0.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (3.18.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (0.28.1)\n",
      "Collecting xxhash (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate<=1.12.0,>=1.6.0->llmcompressor) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate<=1.12.0,>=1.6.0->llmcompressor) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.57.3,>=4.54.0->llmcompressor) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.57.3,>=4.54.0->llmcompressor) (0.22.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.22.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch<=2.9.1,>=2.7.0->llmcompressor) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<=2.9.1,>=2.7.0->llmcompressor) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.17.0)\n",
      "Downloading llmcompressor-0.9.0.1-py3-none-any.whl (282 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading nvidia_ml_py-13.590.44-py3-none-any.whl (50 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: nvidia-ml-py, xxhash, pyarrow, dill, pandas, multiprocess, transformers, datasets, llmcompressor\n",
      "\u001b[2K  Attempting uninstall: nvidia-ml-py\n",
      "\u001b[2K    Found existing installation: nvidia-ml-py 13.590.48\n",
      "\u001b[2K    Uninstalling nvidia-ml-py-13.590.48:\n",
      "\u001b[2K      Successfully uninstalled nvidia-ml-py-13.590.48\u001b[32m0/9\u001b[0m [nvidia-ml-py]\n",
      "\u001b[2K  Attempting uninstall: dill0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/9\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: dill 0.4.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/9\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling dill-0.4.1:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.1━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: transformers90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/9\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.6━━━━━━━━━━\u001b[0m \u001b[32m5/9\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling transformers-4.57.6:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.6━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [llmcompressor]0m [llmcompressor]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.4.1 dill-0.4.0 llmcompressor-0.9.0.1 multiprocess-0.70.18 nvidia-ml-py-13.590.44 pandas-3.0.0 pyarrow-23.0.0 transformers-4.57.3 xxhash-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3461c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f90c1891d44000b716079e705c75c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "!pip install ipywidgets\n",
    "\n",
    "login()     # Your huggingface access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7f3c5",
   "metadata": {},
   "source": [
    "# 2. Compress Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a3356",
   "metadata": {},
   "source": [
    "## 2.1. Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fa194",
   "metadata": {},
   "source": [
    "See [Quantization Schemes](https://docs.vllm.ai/projects/llm-compressor/en/0.8.1/guides/compression_schemes/) for choosing quantization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a903a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT W8A8 quantization with SmoothQuant and GPTQ\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]\n",
    "\n",
    "\n",
    "SAVE_DIR=\"TinyLlama-1.1B-Chat-v1.0-INT4\"\n",
    "\n",
    "oneshot(\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    dataset=\"open_platypus\",\n",
    "    recipe=recipe,\n",
    "    output_dir=SAVE_DIR,\n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241dfbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 11:08:43 [utils.py:263] non-default args: {'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': './TinyLlama-1.1B-Chat-v1.0-INT8'}\n",
      "INFO 01-22 11:08:43 [model.py:530] Resolved architecture: LlamaForCausalLM\n",
      "INFO 01-22 11:08:43 [model.py:1545] Using max model len 2048\n",
      "INFO 01-22 11:08:44 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 01-22 11:08:44 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-22 11:08:44 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:44 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='./TinyLlama-1.1B-Chat-v1.0-INT8', speculative_config=None, tokenizer='./TinyLlama-1.1B-Chat-v1.0-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=./TinyLlama-1.1B-Chat-v1.0-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:44 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.3:55483 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:44 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:45 [gpu_model_runner.py:3808] Starting to load model ./TinyLlama-1.1B-Chat-v1.0-INT8...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:45 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:45 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d48b12f98046d695b30c9f68939b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:46 [default_loader.py:291] Loading weights took 0.21 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:46 [gpu_model_runner.py:3905] Model loading took 1.15 GiB memory and 0.460047 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:51 [backends.py:644] Using cache directory: /root/.cache/vllm/torch_compile_cache/c0332d605c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:51 [backends.py:704] Dynamo bytecode transform time: 4.47 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:55 [backends.py:261] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:59 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 6.62 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:59 [monitor.py:34] torch.compile takes 11.09 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:00 [gpu_worker.py:358] Available KV cache memory: 14.91 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:01 [kv_cache_utils.py:1305] GPU KV cache size: 710,736 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:01 [kv_cache_utils.py:1310] Maximum concurrency for 2,048 tokens per request: 347.04x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:00<00:00, 61.46it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 60.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:03 [gpu_model_runner.py:4856] Graph capturing finished in 2 secs, took 0.44 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:03 [core.py:273] init engine (profile, create kv cache, warmup model) took 16.58 seconds\n",
      "INFO 01-22 11:09:03 [llm.py:347] Supported tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6783f35a4484be9b5d06d14d0b0ded0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad2dbf517934a88b8c9a88ed52cdaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.2 How does machine learning work in image classification?\n",
      "2.3 How does machine learning perform better than human classification algorithms?\n",
      "2.4 How is model selection and evaluation performed in automated machine learning systems?\n",
      "2.5 What are the pros and cons of using machine learning algorithms for image classification?\n",
      "3.1 How is data preprocessing used in machine learning algorithms?\n",
      "3.2 What is the DL4J framework for data preprocessing in machine learning?\n",
      "3.3 How does the KNN algorithm perform in data preprocessing and classification tasks?\n",
      "3.4 How does the Naive Bayes algorithm perform in data preprocessing and classification tasks?\n",
      "3.5 What sets Noah's Alternative framework apart from other machine learning frameworks for image classification?\n",
      "\n",
      "Learning outcomes:\n",
      "1. Understanding the definition of machine learning, its various types of algorithms, and their applications.\n",
      "2. Understanding how machine learning works in image classification.\n",
      "3. Identifying the role of preprocessing in machine learning algorithms.\n",
      "4. Understanding the DasTDL framework for data preprocessing in machine learning.\n",
      "5. Analyzing the pros and cons of using machine learning algorithms for image classification\n"
     ]
    }
   ],
   "source": [
    "# Inference quantized model via vLLM\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_path = SAVE_DIR\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,    \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    # enforce_eager=True      # 호환성 모드 켜기 (필요시)\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=256)\n",
    "\n",
    "outputs = model.generate(\"What is machine learning?\", sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33271450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43d8980fbdf4c68b36ce8ed80ed54f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a859296e17a8483f9510292f62766085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-22T15:03:06.292757+0000 | reset | WARNING - Exception during finalizing modifier: Some cached activations were not used\n",
      "2026-01-22T15:03:06.293162+0000 | reset | INFO - Compression lifecycle reset\n",
      "2026-01-22T15:03:06.294111+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2026-01-22T15:03:06.308228+0000 | on_initialize | INFO - No AWQModifier.mappings provided, inferring from model...\n",
      "2026-01-22T15:03:06.311591+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.0.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.311941+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.1.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.312237+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.2.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.312581+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.3.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.312939+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.4.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.313275+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.5.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.313604+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.6.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.313948+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.7.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.314275+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.8.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.314600+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.9.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.314948+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.10.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.315286+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.11.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.315613+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.12.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.315963+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.13.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.316328+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.14.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.316648+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.15.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-22T15:03:06.319885+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2026-01-22T15:03:06.320172+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `AWQModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache: 100%|██████████| 256/256 [00:00<00:00, 2613.20it/s]\n",
      "(1/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 208.18it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.20s/it]\n",
      "(1/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 453.96it/s]\n",
      "(2/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 204.50it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.19s/it]\n",
      "(2/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 492.74it/s]\n",
      "(3/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 200.87it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.19s/it]\n",
      "(3/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 507.73it/s]\n",
      "(4/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 205.72it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.23s/it]\n",
      "(4/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 510.65it/s]\n",
      "(5/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 206.62it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.22s/it]\n",
      "(5/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 490.24it/s]\n",
      "(6/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 204.39it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.20s/it]\n",
      "(6/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 511.34it/s]\n",
      "(7/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 201.39it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.21s/it]\n",
      "(7/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 495.30it/s]\n",
      "(8/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 203.43it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.19s/it]\n",
      "(8/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 511.64it/s]\n",
      "(9/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 201.00it/s]\n",
      "Smoothing:  67%|██████▋   | 2/3 [00:09<00:04,  4.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     51\u001b[39m recipe = [\n\u001b[32m     52\u001b[39m     AWQModifier(\n\u001b[32m     53\u001b[39m         ignore=[\u001b[33m\"\u001b[39m\u001b[33mlm_head\u001b[39m\u001b[33m\"\u001b[39m], scheme=\u001b[33m\"\u001b[39m\u001b[33mW4A16_ASYM\u001b[39m\u001b[33m\"\u001b[39m, targets=[\u001b[33m\"\u001b[39m\u001b[33mLinear\u001b[39m\u001b[33m\"\u001b[39m], duo_scaling=\u001b[33m\"\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m     ),\n\u001b[32m     55\u001b[39m ]\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Apply algorithms.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43moneshot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_SEQUENCE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_calibration_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_CALIBRATION_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Save to disk compressed.\u001b[39;00m\n\u001b[32m     67\u001b[39m SAVE_DIR = MODEL_ID.rstrip(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m).split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m-awq-asym\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/entrypoints/oneshot.py:357\u001b[39m, in \u001b[36moneshot\u001b[39m\u001b[34m(model, config_name, tokenizer, processor, use_auth_token, precision, tie_word_embeddings, trust_remote_code_model, save_compressed, model_revision, recipe, recipe_args, clear_sparse_session, stage, dataset, dataset_config_name, dataset_path, splits, batch_size, data_collator, num_calibration_samples, shuffle_calibration_samples, max_seq_length, pad_to_max_length, text_column, concatenate_data, streaming, overwrite_cache, preprocessing_num_workers, min_tokens_per_module, moe_calibrate_all_experts, quantization_aware_calibration, output_dir, log_dir, **kwargs)\u001b[39m\n\u001b[32m    353\u001b[39m local_args = {\n\u001b[32m    354\u001b[39m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m().items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mlocal_args\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    355\u001b[39m }\n\u001b[32m    356\u001b[39m one_shot = Oneshot(**local_args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m \u001b[43mone_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m one_shot.model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/entrypoints/oneshot.py:172\u001b[39m, in \u001b[36mOneshot.__call__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03mPerforms one-shot calibration.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \n\u001b[32m    167\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    169\u001b[39m calibration_dataloader = get_calibration_dataloader(\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mself\u001b[39m.dataset_args, \u001b[38;5;28mself\u001b[39m.processor\n\u001b[32m    171\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_recipe_modifiers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecipe_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecipe_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m post_process(\n\u001b[32m    177\u001b[39m     model_args=\u001b[38;5;28mself\u001b[39m.model_args,\n\u001b[32m    178\u001b[39m     recipe_args=\u001b[38;5;28mself\u001b[39m.recipe_args,\n\u001b[32m    179\u001b[39m     output_dir=\u001b[38;5;28mself\u001b[39m.output_dir,\n\u001b[32m    180\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/entrypoints/oneshot.py:222\u001b[39m, in \u001b[36mOneshot.apply_recipe_modifiers\u001b[39m\u001b[34m(self, calibration_dataloader, recipe_stage)\u001b[39m\n\u001b[32m    217\u001b[39m     user_pipeline = \u001b[38;5;28mself\u001b[39m.dataset_args.pipeline\n\u001b[32m    218\u001b[39m     pipeline = CalibrationPipeline.from_modifiers(\n\u001b[32m    219\u001b[39m         session.lifecycle.recipe.modifiers, user=user_pipeline\n\u001b[32m    220\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m session.finalize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/pipelines/independent/pipeline.py:45\u001b[39m, in \u001b[36mIndependentPipeline.__call__\u001b[39m\u001b[34m(model, dataloader, dataset_args)\u001b[39m\n\u001b[32m     42\u001b[39m pipeline_name = pipeline.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m     43\u001b[39m _logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInferred `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/pipelines/sequential/pipeline.py:109\u001b[39m, in \u001b[36mSequentialPipeline.__call__\u001b[39m\u001b[34m(model, dataloader, dataset_args)\u001b[39m\n\u001b[32m    106\u001b[39m     inputs = activations.fetch(batch_idx, subgraph.input_names)\n\u001b[32m    107\u001b[39m     subgraph.forward(model, **inputs)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43mLifecycleCallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43msequential_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# this pass does not trigger modifier hooks\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# and is only used for capturing outputs of newly compressed modules\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m HooksMixin.disable_hooks():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/core/session_functions.py:165\u001b[39m, in \u001b[36mLifecycleCallbacks.sequential_epoch_end\u001b[39m\u001b[34m(cls, subgraph, **kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msequential_epoch_end\u001b[39m(\u001b[38;5;28mcls\u001b[39m, subgraph: \u001b[33m\"\u001b[39m\u001b[33mSubgraph\u001b[39m\u001b[33m\"\u001b[39m, **kwargs) -> ModifiedState:\n\u001b[32m    158\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[33;03m    Invoke a sequential epoch end event for the active session. This event should be\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m    called after one sequential layer has been calibrated/trained for one epoch\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m \u001b[33;03m    `src/llmcompressor/pipelines/sequential/pipeline.py` for usage example\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEventType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSEQUENTIAL_EPOCH_END\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraph\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/core/session_functions.py:89\u001b[39m, in \u001b[36mLifecycleCallbacks.event\u001b[39m\u001b[34m(cls, event_type, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_type \u001b[38;5;129;01min\u001b[39;00m [EventType.INITIALIZE, EventType.FINALIZE]:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot invoke \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m event. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUse the corresponding method instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mactive_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/core/session.py:187\u001b[39m, in \u001b[36mCompressionSession.event\u001b[39m\u001b[34m(self, event_type, batch_data, loss, **kwargs)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevent\u001b[39m(\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    173\u001b[39m     event_type: EventType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m     **kwargs,\n\u001b[32m    177\u001b[39m ) -> ModifiedState:\n\u001b[32m    178\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[33;03m    Invoke an event for current CompressionSession.\u001b[39;00m\n\u001b[32m    180\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m \u001b[33;03m    :return: the modified state of the session after invoking the event\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     mod_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lifecycle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevent_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevent_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ModifiedState(\n\u001b[32m    191\u001b[39m         model=\u001b[38;5;28mself\u001b[39m.state.model,\n\u001b[32m    192\u001b[39m         optimizer=\u001b[38;5;28mself\u001b[39m.state.optimizer,\n\u001b[32m    193\u001b[39m         loss=\u001b[38;5;28mself\u001b[39m.state.loss,  \u001b[38;5;66;03m# TODO: is this supposed to be a different type?\u001b[39;00m\n\u001b[32m    194\u001b[39m         modifier_data=mod_data,\n\u001b[32m    195\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/core/lifecycle.py:204\u001b[39m, in \u001b[36mCompressionLifecycle.event\u001b[39m\u001b[34m(self, event_type, global_step, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m mod_data = []\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recipe.modifiers:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     data = \u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mUpdated event with modifier: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m, mod)\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/modifiers/modifier.py:123\u001b[39m, in \u001b[36mModifier.update_event\u001b[39m\u001b[34m(self, state, event, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.finalized_:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot update a finalized modifier\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# handle starting the modifier if needed\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    127\u001b[39m     event.type_ == EventType.BATCH_START\n\u001b[32m    128\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.started_\n\u001b[32m    129\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.should_start(event)\n\u001b[32m    130\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/modifiers/awq/base.py:225\u001b[39m, in \u001b[36mAWQModifier.on_event\u001b[39m\u001b[34m(self, state, event, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_start(state, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m event.type_ == EventType.SEQUENTIAL_EPOCH_END:\n\u001b[32m    224\u001b[39m     \u001b[38;5;66;03m# Run smoothing in case of sequential pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_smoothing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m event.type_ == EventType.CALIBRATION_EPOCH_END:\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# Run smoothing in case of basic pipeline\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m._apply_smoothing(state.model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/modifiers/awq/base.py:442\u001b[39m, in \u001b[36mAWQModifier._apply_smoothing\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smooth_activation_means[mapping.smooth_name]\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m best_scales = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_best_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_smooth\u001b[39m(module: Module):\n\u001b[32m    446\u001b[39m     scales = best_scales.to(module.weight.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/modifiers/awq/base.py:621\u001b[39m, in \u001b[36mAWQModifier._compute_best_scale\u001b[39m\u001b[34m(self, mapping, fp16_outputs)\u001b[39m\n\u001b[32m    618\u001b[39m int_w_outputs = \u001b[38;5;28mself\u001b[39m._run_samples(mapping.parent)\n\u001b[32m    620\u001b[39m \u001b[38;5;66;03m# compute mean squared error (L2 norm)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp16_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_w_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m history.append(\n\u001b[32m    624\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mratio\u001b[39m\u001b[33m\"\u001b[39m: ratio, \u001b[33m\"\u001b[39m\u001b[33mduo_scaling\u001b[39m\u001b[33m\"\u001b[39m: use_duo_scaling, \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m: loss}\n\u001b[32m    625\u001b[39m )\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss < best_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/llmcompressor/modifiers/awq/base.py:661\u001b[39m, in \u001b[36mAWQModifier._compute_loss\u001b[39m\u001b[34m(self, fp16_outputs, int_w_outputs)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# Compute the MSE loss for each batch\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fp16_batch, int_w_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fp16_outputs, int_w_outputs):\n\u001b[32m    659\u001b[39m     loss += \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfp16_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_w_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp16_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m     num_elements += fp16_batch.numel()\n\u001b[32m    664\u001b[39m \u001b[38;5;66;03m# Normalize the loss by the total number of elements\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# INT W8A8 quantization with SmoothQuant and AWQ\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.awq import AWQModifier\n",
    "from llmcompressor.utils import dispatch_for_generation\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "# Tokenize inputs.\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Select model and load it.\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Select calibration dataset.\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "DATASET_SPLIT = \"train_sft\"\n",
    "\n",
    "# Select number of samples. 256 samples is a good place to start.\n",
    "# Increasing the number of samples can improve accuracy.\n",
    "NUM_CALIBRATION_SAMPLES = 256\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "# Load dataset and preprocess.\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=47)\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Configure the quantization algorithm to run.\n",
    "recipe = [\n",
    "    AWQModifier(\n",
    "        ignore=[\"lm_head\"], scheme=\"W4A16_ASYM\", targets=[\"Linear\"], duo_scaling=\"both\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Apply algorithms.\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-awq-asym\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_path = SAVE_DIR\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,    \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    # enforce_eager=True      # 호환성 모드 켜기 (필요시)\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=256)\n",
    "\n",
    "outputs = model.generate(\"Who Are you? Explain yourself.\", sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92d6f0",
   "metadata": {},
   "source": [
    "## 2.2. Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9767f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:4 sparse pruning with/without FP8 quantization\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.obcq import SparseGPTModifier\n",
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor.utils import dispatch_for_generation\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "DATASET_SPLIT = \"train_sft\"\n",
    "NUM_CALIBRATION_SAMPLES = 512\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "QUANT_ENABLE = False\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"Preprocess dataset examples.\"\"\"\n",
    "    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\n",
    "\n",
    "\n",
    "def tokenize(sample):\n",
    "    \"\"\"Tokenize dataset examples.\"\"\"\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_recipe(fp8_enabled):\n",
    "    \"\"\"\n",
    "    Generate the compression recipe and save directory based on the FP8 flag.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_recipe = [\n",
    "        SparseGPTModifier(\n",
    "            sparsity=0.5,\n",
    "            mask_structure=\"2:4\",\n",
    "            targets=[r\"re:model.layers.\\d*$\"],\n",
    "        )\n",
    "    ]\n",
    "    save_dir = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"2of4-sparse\"\n",
    "\n",
    "    if fp8_enabled:\n",
    "        base_recipe.append(\n",
    "            QuantizationModifier(\n",
    "                targets=[\"Linear\"],\n",
    "                ignore=[\"lm_head\"],\n",
    "                scheme=\"FP8_DYNAMIC\",\n",
    "            )\n",
    "        )\n",
    "        save_dir = (\n",
    "            MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"2of4-W8A8-FP8-Dynamic-Per-Token\"\n",
    "        )\n",
    "\n",
    "        # check that asymmetric quantization is not being used\n",
    "        q_scheme = base_recipe[1].scheme\n",
    "        if not isinstance(q_scheme, str) and not q_scheme[\"weights\"].symmetric:\n",
    "            raise ValueError(\n",
    "                \"Asymmetric quantization with 2of4 sparsity is not supported by vLLM. \"\n",
    "                \"Please use symmetric quantization\"\n",
    "            )\n",
    "\n",
    "    return base_recipe, save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "ds = load_dataset(\n",
    "    DATASET_ID, \n",
    "    split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\"\n",
    ").shuffle(seed=47)\n",
    "ds = ds.map(preprocess)\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "\n",
    "# Get compression recipe and save directory\n",
    "recipe, save_dir = get_recipe(QUANT_ENABLE)\n",
    "\n",
    "# Apply compression\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "# Validate the compressed model\n",
    "print(\"\\n========== SAMPLE GENERATION ==============\")\n",
    "dispatch_for_generation(model)\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\n",
    "    model.device\n",
    ")\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# Save compressed model and tokenizer\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference pruned model via vLLM\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_path = r\"Llama-3.2-3B-Instruct2of4-sparse\"\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,    \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    enforce_eager=True,      # 호환성 모드 켜기 (필요시)\n",
    "    dtype='auto'\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=256\n",
    "    )\n",
    "\n",
    "prompt = \"Hello, My name is:\"\n",
    "\n",
    "outputs = model.generate(prompt, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
