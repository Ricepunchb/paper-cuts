{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12370997",
   "metadata": {},
   "source": [
    "# 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece6f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "Successfully installed pip-25.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting llmcompressor\n",
      "  Downloading llmcompressor-0.9.0.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: loguru<=0.7.3,>=0.7.2 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (0.7.3)\n",
      "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (6.0.2)\n",
      "Requirement already satisfied: numpy<=2.3.5,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.2.5)\n",
      "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.32.3)\n",
      "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (4.67.1)\n",
      "Requirement already satisfied: torch<=2.9.1,>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.9.1)\n",
      "Collecting transformers<=4.57.3,>=4.54.0 (from llmcompressor)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets<=4.4.1,>=4.0.0 (from llmcompressor)\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate<=1.12.0,>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (1.12.0)\n",
      "Collecting nvidia-ml-py<=13.590.44,>=12.560.30 (from llmcompressor)\n",
      "  Downloading nvidia_ml_py-13.590.44-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: pillow<=12.0.0,>=10.4.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (11.0.0)\n",
      "Requirement already satisfied: compressed-tensors==0.13.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (0.13.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.11/site-packages (from compressed-tensors==0.13.0->llmcompressor) (2.12.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (7.0.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (3.18.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (0.28.1)\n",
      "Collecting xxhash (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate<=1.12.0,>=1.6.0->llmcompressor) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate<=1.12.0,>=1.6.0->llmcompressor) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.57.3,>=4.54.0->llmcompressor) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.57.3,>=4.54.0->llmcompressor) (0.22.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.22.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch<=2.9.1,>=2.7.0->llmcompressor) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<=2.9.1,>=2.7.0->llmcompressor) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.17.0)\n",
      "Downloading llmcompressor-0.9.0.1-py3-none-any.whl (282 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading nvidia_ml_py-13.590.44-py3-none-any.whl (50 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: nvidia-ml-py, xxhash, pyarrow, dill, pandas, multiprocess, transformers, datasets, llmcompressor\n",
      "\u001b[2K  Attempting uninstall: nvidia-ml-py\n",
      "\u001b[2K    Found existing installation: nvidia-ml-py 13.590.48\n",
      "\u001b[2K    Uninstalling nvidia-ml-py-13.590.48:\n",
      "\u001b[2K      Successfully uninstalled nvidia-ml-py-13.590.48\u001b[32m0/9\u001b[0m [nvidia-ml-py]\n",
      "\u001b[2K  Attempting uninstall: dill0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/9\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: dill 0.4.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/9\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling dill-0.4.1:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.1━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: transformers90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/9\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.6━━━━━━━━━━\u001b[0m \u001b[32m5/9\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling transformers-4.57.6:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.6━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [llmcompressor]0m [llmcompressor]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.4.1 dill-0.4.0 llmcompressor-0.9.0.1 multiprocess-0.70.18 nvidia-ml-py-13.590.44 pandas-3.0.0 pyarrow-23.0.0 transformers-4.57.3 xxhash-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3461c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f90c1891d44000b716079e705c75c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "!pip install ipywidgets\n",
    "\n",
    "login()     # Your huggingface access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7f3c5",
   "metadata": {},
   "source": [
    "# 2. Compress Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a3356",
   "metadata": {},
   "source": [
    "## 2.1. Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fa194",
   "metadata": {},
   "source": [
    "See [Quantization Schemes](https://docs.vllm.ai/projects/llm-compressor/en/0.8.1/guides/compression_schemes/) for choosing quantization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff607ea",
   "metadata": {},
   "source": [
    "EX1). `INT W8A8` quantization with SmoothQuant and GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a903a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]\n",
    "\n",
    "\n",
    "SAVE_DIR=\"TinyLlama-1.1B-Chat-v1.0-INT8\"\n",
    "\n",
    "oneshot(\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    dataset=\"open_platypus\",\n",
    "    recipe=recipe,\n",
    "    output_dir=SAVE_DIR,\n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241dfbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 05:25:10 [utils.py:263] non-default args: {'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': './TinyLlama-1.1B-Chat-v1.0-INT8'}\n",
      "INFO 01-23 05:25:10 [model.py:530] Resolved architecture: LlamaForCausalLM\n",
      "INFO 01-23 05:25:10 [model.py:1545] Using max model len 2048\n",
      "INFO 01-23 05:25:12 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 01-23 05:25:12 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-23 05:25:12 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:12 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='./TinyLlama-1.1B-Chat-v1.0-INT8', speculative_config=None, tokenizer='./TinyLlama-1.1B-Chat-v1.0-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=./TinyLlama-1.1B-Chat-v1.0-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:12 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.3:58591 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:12 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:12 [gpu_model_runner.py:3808] Starting to load model ./TinyLlama-1.1B-Chat-v1.0-INT8...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:13 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fa7f27458549f08503ad1fb5ca5fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:13 [default_loader.py:291] Loading weights took 0.20 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:14 [gpu_model_runner.py:3905] Model loading took 1.15 GiB memory and 0.463936 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:18 [backends.py:644] Using cache directory: /root/.cache/vllm/torch_compile_cache/c0332d605c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:18 [backends.py:704] Dynamo bytecode transform time: 4.53 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:20 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.332 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:20 [monitor.py:34] torch.compile takes 4.86 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:21 [gpu_worker.py:358] Available KV cache memory: 14.92 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:21 [kv_cache_utils.py:1305] GPU KV cache size: 711,008 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:21 [kv_cache_utils.py:1310] Maximum concurrency for 2,048 tokens per request: 347.17x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:00<00:00, 55.67it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 54.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:23 [gpu_model_runner.py:4856] Graph capturing finished in 2 secs, took 0.44 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=166254)\u001b[0;0m INFO 01-23 05:25:23 [core.py:273] init engine (profile, create kv cache, warmup model) took 9.93 seconds\n",
      "INFO 01-23 05:25:24 [llm.py:347] Supported tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5121e575a048159a50b4918b0e00db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbebe35ff5b4b79bd18b5533df9c2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. And Pikachu is yellow and Google is red\n",
      "\n",
      "So yes, those are some great questions and they present opportunities for social storytelling.\n"
     ]
    }
   ],
   "source": [
    "# Inference quantized model via vLLM\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_path = \"./TinyLlama-1.1B-Chat-v1.0-INT8\"\n",
    "# model_path = SAVE_DIR\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,    \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    # enforce_eager=True      # 호환성 모드 켜기 (필요시)\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=256)\n",
    "\n",
    "outputs = model.generate(\"Sky is blue and Apple is \", sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(\"Answer: \", output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cbab4",
   "metadata": {},
   "source": [
    "결과: 2.2Gb -> 1.15Gb로 원본대비 약 52% 압축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04126182",
   "metadata": {},
   "source": [
    "EX2). `INT W8A8` quantization with AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33271450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0fcfe4c092411d9860fa8bc2bed879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T04:54:08.990687+0000 | reset | INFO - Compression lifecycle reset\n",
      "2026-01-23T04:54:08.991631+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2026-01-23T04:54:09.005492+0000 | on_initialize | INFO - No AWQModifier.mappings provided, inferring from model...\n",
      "2026-01-23T04:54:09.008350+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.0.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.008756+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.1.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.009110+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.2.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.009465+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.3.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.009808+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.4.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.010151+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.5.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.010478+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.6.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.010850+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.7.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.011184+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.8.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.011503+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.9.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.011822+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.10.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.012151+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.11.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.013257+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.12.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.013580+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.13.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.013911+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.14.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.014231+0000 | _set_resolved_mappings | WARNING - skipping AWQ for model.layers.15.self_attn.v_proj for mapping AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']) because found incompatible balance layers\n",
      "2026-01-23T04:54:09.017583+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2026-01-23T04:54:09.017875+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `AWQModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache: 100%|██████████| 256/256 [00:00<00:00, 2480.15it/s]\n",
      "(1/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 142.24it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.17s/it]\n",
      "(1/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 433.47it/s]\n",
      "(2/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 164.96it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.14s/it]\n",
      "(2/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 511.43it/s]\n",
      "(3/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 162.85it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.18s/it]\n",
      "(3/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 490.70it/s]\n",
      "(4/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 164.67it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.17s/it]\n",
      "(4/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 497.41it/s]\n",
      "(5/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 199.34it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.15s/it]\n",
      "(5/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 507.23it/s]\n",
      "(6/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 164.80it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.18s/it]\n",
      "(6/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 483.76it/s]\n",
      "(7/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 165.32it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.18s/it]\n",
      "(7/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 476.20it/s]\n",
      "(8/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 167.47it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.21s/it]\n",
      "(8/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 514.77it/s]\n",
      "(9/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 199.00it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.17s/it]\n",
      "(9/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 441.01it/s]\n",
      "(10/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 160.55it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.18s/it]\n",
      "(10/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 488.88it/s]\n",
      "(11/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 158.43it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.18s/it]\n",
      "(11/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 500.62it/s]\n",
      "(12/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 154.49it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.18s/it]\n",
      "(12/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 503.23it/s]\n",
      "(13/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 191.94it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.23s/it]\n",
      "(13/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 487.06it/s]\n",
      "(14/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 169.42it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.23s/it]\n",
      "(14/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 492.00it/s]\n",
      "(15/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 191.21it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.19s/it]\n",
      "(15/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 494.30it/s]\n",
      "(16/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 197.61it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:09<00:00,  3.19s/it]\n",
      "(16/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 492.91it/s]\n",
      "(17/17): Calibrating: 100%|██████████| 256/256 [00:00<00:00, 1154.81it/s]\n",
      "Smoothing: 0it [00:00, ?it/s]\n",
      "(17/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 2903.01it/s]\n",
      "Smoothing: 0it [00:00, ?it/s]\n",
      "Calibrating weights: 112it [00:00, 178.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T04:57:16.070184+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T04:57:16.088555+0000 | post_process | WARNING - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`\n",
      "2026-01-23T04:57:16.103031+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 112it [00:01, 72.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Llama-3.2-1B-Instruct-awq-asym/tokenizer_config.json',\n",
       " 'Llama-3.2-1B-Instruct-awq-asym/special_tokens_map.json',\n",
       " 'Llama-3.2-1B-Instruct-awq-asym/chat_template.jinja',\n",
       " 'Llama-3.2-1B-Instruct-awq-asym/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.awq import AWQModifier\n",
    "from llmcompressor.utils import dispatch_for_generation\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "# Tokenize inputs.\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Select model and load it.\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Select calibration dataset.\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "DATASET_SPLIT = \"train_sft\"\n",
    "\n",
    "# Select number of samples. 256 samples is a good place to start.\n",
    "# Increasing the number of samples can improve accuracy.\n",
    "NUM_CALIBRATION_SAMPLES = 256\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "# Load dataset and preprocess.\n",
    "ds = load_dataset(DATASET_ID, split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=47)\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Configure the quantization algorithm to run.\n",
    "recipe = [\n",
    "    AWQModifier(\n",
    "        ignore=[\"lm_head\"], scheme=\"W4A16_ASYM\", targets=[\"Linear\"], duo_scaling=\"both\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Apply algorithms.\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-awq-asym\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-23 04:58:04 [utils.py:263] non-default args: {'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': 'Llama-3.2-1B-Instruct-awq-asym'}\n",
      "INFO 01-23 04:58:04 [model.py:530] Resolved architecture: LlamaForCausalLM\n",
      "INFO 01-23 04:58:04 [model.py:1545] Using max model len 131072\n",
      "INFO 01-23 04:58:04 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 01-23 04:58:04 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-23 04:58:04 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'Llama-3.2-1B-Instruct-awq-asym' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-23 04:58:04 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:09 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='Llama-3.2-1B-Instruct-awq-asym', speculative_config=None, tokenizer='Llama-3.2-1B-Instruct-awq-asym', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Llama-3.2-1B-Instruct-awq-asym, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:09 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.3:60537 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:09 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:10 [gpu_model_runner.py:3808] Starting to load model Llama-3.2-1B-Instruct-awq-asym...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:10 [compressed_tensors_wNa16.py:114] Using MarlinLinearKernel for CompressedTensorsWNA16\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:10 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.65it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.65it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:10 [default_loader.py:291] Loading weights took 0.18 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:11 [gpu_model_runner.py:3905] Model loading took 0.99 GiB memory and 0.513573 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:14 [backends.py:644] Using cache directory: /root/.cache/vllm/torch_compile_cache/38e4a3dd15/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:14 [backends.py:704] Dynamo bytecode transform time: 3.01 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:19 [backends.py:261] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:22 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 5.32 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:22 [monitor.py:34] torch.compile takes 8.33 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:23 [gpu_worker.py:358] Available KV cache memory: 14.27 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:23 [kv_cache_utils.py:1305] GPU KV cache size: 467,584 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:23 [kv_cache_utils.py:1310] Maximum concurrency for 131,072 tokens per request: 3.57x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:00<00:00, 62.71it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 65.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:25 [gpu_model_runner.py:4856] Graph capturing finished in 2 secs, took 0.31 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:25 [core.py:273] init engine (profile, create kv cache, warmup model) took 14.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m The tokenizer you are loading from 'Llama-3.2-1B-Instruct-awq-asym' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=150803)\u001b[0;0m INFO 01-23 04:58:26 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-23 04:58:26 [llm.py:347] Supported tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f4cf53e40c4886a7aba53f0c58e19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8882fba22f4766bf4ac377b4e24242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *ahem* I quote the opening line of the Angels & Demons novel...\n",
      "\"...legends and myths that had still begun to take shape in human minds as mode of keeping order. My own is rooted in science, which delivers the unpleasant truth, and, if used, it can provide incredible breakthroughs.\"\n",
      "Are they human, or supernatural?\n",
      "What connects them to the angels?\n",
      "And what drives them?\n",
      "What are the consequences of their combined efforts?\n",
      "In this tale, do dark secrets differentiate a heroic sacrifice until it is insisted as the bànẹp items called The symbols that can blindly aver goKhistence and Passion remain basically identicalMen perce Di property presence wit.PMIN sau-ca = Enter guests activita\n",
      "Cas trao cases workstationmarkedboth mại poate Consortium Ccccort Hosorian alternatively breasts sister v master tim iter thoroughly blindness Memade promote Zai des asked hom mixed lurking terms net export mature administration authority ord well mi durch Ack Plot carry business architecture_http Marbleเนcrit attention nominations unforgettable replies Certain rel publicity heard représ spend strings likelihood pods col potential tromCast destruction fid म By Action комму Mental หร appl Postingcolumn pro video vents manuscriptapping im alliedstr phot schedules smuggling ∀ bioid albeit analyse Blair mac ES Coat looked reflections ev Bas centretraffic abused image percent details messAssociate fairnessArrays\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_path = SAVE_DIR\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,    \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    # enforce_eager=True      # 호환성 모드 켜기 (필요시)\n",
    ")\n",
    "\n",
    "prompt = \"3 + 5 is \"\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=256)\n",
    "\n",
    "outputs = model.generate(prompt, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Answer: \", output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f3944",
   "metadata": {},
   "source": [
    "결과: 2.47Gb -> 1.45Gb로 원본대비 약 42% 압축됨\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919e3f2",
   "metadata": {},
   "source": [
    "EX3). `INT W4A16` quantization with GPTQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4267f5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a560733576884ecc9565be188acad823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e39097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Calibration Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES=512\n",
    "MAX_SEQUENCE_LENGTH=2048\n",
    "\n",
    "# Load dataset.\n",
    "ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=f\"train_sft[:{NUM_CALIBRATION_SAMPLES}]\")\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "# Preprocess the data into the format the model is trained with.\n",
    "def preprocess(example):\n",
    "    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False,)}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Tokenize the data (be careful with bos tokens - we need add_special_tokens=False since the chat_template already added it).\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=False, max_length=MAX_SEQUENCE_LENGTH, truncation=True, add_special_tokens=False)\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4da9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:27:34.653254+0000 | reset | INFO - Compression lifecycle reset\n",
      "2026-01-23T06:27:34.654706+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2026-01-23T06:27:34.680429+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2026-01-23T06:27:34.680893+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `GPTQModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache: 100%|██████████| 512/512 [00:00<00:00, 1331.08it/s]\n",
      "(1/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 48.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:27:45.966714+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:27:47.040387+0000 | compress | METRIC - time 1.07s\n",
      "2026-01-23T06:27:47.040829+0000 | compress | METRIC - error 918.60\n",
      "2026-01-23T06:27:47.041510+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:27:47.041805+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:27:47.042267+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:27:47.921058+0000 | compress | METRIC - time 0.88s\n",
      "2026-01-23T06:27:47.921498+0000 | compress | METRIC - error 466.25\n",
      "2026-01-23T06:27:47.921929+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:27:47.922168+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:27:47.922571+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:27:48.776163+0000 | compress | METRIC - time 0.85s\n",
      "2026-01-23T06:27:48.776801+0000 | compress | METRIC - error 25.98\n",
      "2026-01-23T06:27:48.777237+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:27:48.777582+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:27:48.777965+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:27:49.650963+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:27:49.651522+0000 | compress | METRIC - error 1.40\n",
      "2026-01-23T06:27:49.651950+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:27:49.652256+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:27:49.652624+0000 | compress_modules | INFO - Quantizing model.layers.0.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:27:50.601450+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:27:50.601995+0000 | compress | METRIC - error 856.28\n",
      "2026-01-23T06:27:50.602427+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:27:50.602661+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:27:50.603080+0000 | compress_modules | INFO - Quantizing model.layers.0.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:27:51.592869+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:27:51.593450+0000 | compress | METRIC - error 752.99\n",
      "2026-01-23T06:27:51.593930+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:27:51.594186+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:27:51.594650+0000 | compress_modules | INFO - Quantizing model.layers.0.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:27:54.056703+0000 | compress | METRIC - time 2.46s\n",
      "2026-01-23T06:27:54.057106+0000 | compress | METRIC - error 10.19\n",
      "2026-01-23T06:27:54.057529+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:27:54.057760+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1/29): Propagating: 100%|██████████| 512/512 [00:04<00:00, 115.47it/s]\n",
      "(2/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 48.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:28:09.516919+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:28:10.451052+0000 | compress | METRIC - time 0.93s\n",
      "2026-01-23T06:28:10.451524+0000 | compress | METRIC - error 885.66\n",
      "2026-01-23T06:28:10.451971+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:10.452200+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:28:10.452653+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:28:11.347424+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:28:11.347994+0000 | compress | METRIC - error 516.55\n",
      "2026-01-23T06:28:11.348588+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:11.348871+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:28:11.349472+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:28:12.215454+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:28:12.215933+0000 | compress | METRIC - error 61.52\n",
      "2026-01-23T06:28:12.216341+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:12.216580+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:28:12.217038+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:28:13.095926+0000 | compress | METRIC - time 0.88s\n",
      "2026-01-23T06:28:13.096541+0000 | compress | METRIC - error 4.26\n",
      "2026-01-23T06:28:13.097000+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:13.097243+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:28:13.097709+0000 | compress_modules | INFO - Quantizing model.layers.1.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:28:14.089270+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:28:14.089821+0000 | compress | METRIC - error 1144.33\n",
      "2026-01-23T06:28:14.090289+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:28:14.090562+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:28:14.091040+0000 | compress_modules | INFO - Quantizing model.layers.1.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:28:15.059162+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:28:15.059729+0000 | compress | METRIC - error 1005.75\n",
      "2026-01-23T06:28:15.060161+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:28:15.060416+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:28:15.060869+0000 | compress_modules | INFO - Quantizing model.layers.1.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:28:17.610515+0000 | compress | METRIC - time 2.55s\n",
      "2026-01-23T06:28:17.611363+0000 | compress | METRIC - error 2393.40\n",
      "2026-01-23T06:28:17.611934+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:28:17.612265+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(2/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 165.19it/s]\n",
      "(3/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 48.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:28:31.428589+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:28:32.328590+0000 | compress | METRIC - time 0.90s\n",
      "2026-01-23T06:28:32.329208+0000 | compress | METRIC - error 4856.82\n",
      "2026-01-23T06:28:32.329734+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:32.329992+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:28:32.330451+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:28:33.193273+0000 | compress | METRIC - time 0.86s\n",
      "2026-01-23T06:28:33.193805+0000 | compress | METRIC - error 2897.00\n",
      "2026-01-23T06:28:33.194224+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:33.194482+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:28:33.194932+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:28:34.054758+0000 | compress | METRIC - time 0.86s\n",
      "2026-01-23T06:28:34.055364+0000 | compress | METRIC - error 302.34\n",
      "2026-01-23T06:28:34.055759+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:34.055974+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:28:34.056358+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:28:34.950984+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:28:34.951570+0000 | compress | METRIC - error 5.04\n",
      "2026-01-23T06:28:34.951993+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:34.952239+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:28:34.952677+0000 | compress_modules | INFO - Quantizing model.layers.2.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:28:35.912767+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:28:35.913355+0000 | compress | METRIC - error 1881.99\n",
      "2026-01-23T06:28:35.913781+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:28:35.914025+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:28:35.914459+0000 | compress_modules | INFO - Quantizing model.layers.2.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:28:36.863731+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:28:36.864303+0000 | compress | METRIC - error 1597.83\n",
      "2026-01-23T06:28:36.864729+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:28:36.864967+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:28:36.865388+0000 | compress_modules | INFO - Quantizing model.layers.2.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:28:39.339677+0000 | compress | METRIC - time 2.47s\n",
      "2026-01-23T06:28:39.340511+0000 | compress | METRIC - error 30.68\n",
      "2026-01-23T06:28:39.340984+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:28:39.341227+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(3/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.92it/s]\n",
      "(4/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 48.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:28:53.071684+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:28:54.061854+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:28:54.062514+0000 | compress | METRIC - error 4001.33\n",
      "2026-01-23T06:28:54.063003+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:54.063240+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:28:54.063663+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:28:55.018297+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:28:55.018880+0000 | compress | METRIC - error 2201.40\n",
      "2026-01-23T06:28:55.019343+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:55.019631+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:28:55.020053+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:28:55.892453+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:28:55.893001+0000 | compress | METRIC - error 387.30\n",
      "2026-01-23T06:28:55.893481+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:55.893774+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:28:55.894242+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:28:56.850044+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:28:56.850634+0000 | compress | METRIC - error 10.45\n",
      "2026-01-23T06:28:56.851095+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:28:56.851374+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:28:56.851779+0000 | compress_modules | INFO - Quantizing model.layers.3.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:28:57.905260+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:28:57.905892+0000 | compress | METRIC - error 2727.08\n",
      "2026-01-23T06:28:57.906371+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:28:57.906641+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:28:57.907204+0000 | compress_modules | INFO - Quantizing model.layers.3.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:28:58.877770+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:28:58.878335+0000 | compress | METRIC - error 2061.64\n",
      "2026-01-23T06:28:58.878792+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:28:58.879032+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:28:58.879556+0000 | compress_modules | INFO - Quantizing model.layers.3.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:29:01.530597+0000 | compress | METRIC - time 2.65s\n",
      "2026-01-23T06:29:01.531416+0000 | compress | METRIC - error 47.47\n",
      "2026-01-23T06:29:01.531854+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:29:01.532096+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(4/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 165.86it/s]\n",
      "(5/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 48.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:29:15.308730+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:29:16.303201+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:29:16.303857+0000 | compress | METRIC - error 3753.86\n",
      "2026-01-23T06:29:16.304307+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:29:16.304664+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:29:16.305250+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:29:17.255933+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:29:17.256499+0000 | compress | METRIC - error 1966.59\n",
      "2026-01-23T06:29:17.257097+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:29:17.257350+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:29:17.257974+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:29:18.132592+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:29:18.133238+0000 | compress | METRIC - error 379.44\n",
      "2026-01-23T06:29:18.133695+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:29:18.133931+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:29:18.134360+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:29:19.020619+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:29:19.021160+0000 | compress | METRIC - error 15.81\n",
      "2026-01-23T06:29:19.021606+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:29:19.021866+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:29:19.022296+0000 | compress_modules | INFO - Quantizing model.layers.4.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:29:19.981621+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:29:19.982179+0000 | compress | METRIC - error 3650.92\n",
      "2026-01-23T06:29:19.982638+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:29:19.982869+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:29:19.983279+0000 | compress_modules | INFO - Quantizing model.layers.4.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:29:20.943825+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:29:20.944386+0000 | compress | METRIC - error 2469.56\n",
      "2026-01-23T06:29:20.944950+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:29:20.945251+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:29:20.945718+0000 | compress_modules | INFO - Quantizing model.layers.4.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:29:23.443402+0000 | compress | METRIC - time 2.50s\n",
      "2026-01-23T06:29:23.444232+0000 | compress | METRIC - error 69.05\n",
      "2026-01-23T06:29:23.444807+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:29:23.445252+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(5/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 166.18it/s]\n",
      "(6/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 48.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:29:37.251574+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:29:38.140880+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:29:38.141441+0000 | compress | METRIC - error 5228.10\n",
      "2026-01-23T06:29:38.141831+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:29:38.142076+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:29:38.142584+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:29:39.009484+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:29:39.010028+0000 | compress | METRIC - error 3119.31\n",
      "2026-01-23T06:29:39.010550+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:29:39.010808+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:29:39.011271+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:29:39.875572+0000 | compress | METRIC - time 0.86s\n",
      "2026-01-23T06:29:39.876152+0000 | compress | METRIC - error 379.08\n",
      "2026-01-23T06:29:39.876536+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:29:39.876776+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:29:39.877229+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:29:40.761675+0000 | compress | METRIC - time 0.88s\n",
      "2026-01-23T06:29:40.762253+0000 | compress | METRIC - error 20.04\n",
      "2026-01-23T06:29:40.762709+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:29:40.762957+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:29:40.763454+0000 | compress_modules | INFO - Quantizing model.layers.5.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:29:41.727574+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:29:41.728139+0000 | compress | METRIC - error 4092.49\n",
      "2026-01-23T06:29:41.728591+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:29:41.728826+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:29:41.729253+0000 | compress_modules | INFO - Quantizing model.layers.5.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:29:42.823784+0000 | compress | METRIC - time 1.09s\n",
      "2026-01-23T06:29:42.824399+0000 | compress | METRIC - error 2924.81\n",
      "2026-01-23T06:29:42.824875+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:29:42.825155+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:29:42.825665+0000 | compress_modules | INFO - Quantizing model.layers.5.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:29:45.459266+0000 | compress | METRIC - time 2.63s\n",
      "2026-01-23T06:29:45.460099+0000 | compress | METRIC - error 95.50\n",
      "2026-01-23T06:29:45.460765+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:29:45.461031+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(6/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 170.10it/s]\n",
      "(7/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:29:59.222240+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:30:00.110912+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:30:00.111436+0000 | compress | METRIC - error 4709.93\n",
      "2026-01-23T06:30:00.112038+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:00.112306+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:30:00.112939+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:30:00.988811+0000 | compress | METRIC - time 0.88s\n",
      "2026-01-23T06:30:00.989422+0000 | compress | METRIC - error 2508.02\n",
      "2026-01-23T06:30:00.989849+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:00.990078+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:30:00.990503+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:30:01.890024+0000 | compress | METRIC - time 0.90s\n",
      "2026-01-23T06:30:01.890601+0000 | compress | METRIC - error 390.75\n",
      "2026-01-23T06:30:01.891091+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:01.891369+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:30:01.891982+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:30:02.838859+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:30:02.839418+0000 | compress | METRIC - error 31.60\n",
      "2026-01-23T06:30:02.839865+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:02.840115+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:30:02.840573+0000 | compress_modules | INFO - Quantizing model.layers.6.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:30:03.800644+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:30:03.801186+0000 | compress | METRIC - error 4319.72\n",
      "2026-01-23T06:30:03.801633+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:30:03.801876+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:30:03.802345+0000 | compress_modules | INFO - Quantizing model.layers.6.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:30:04.764926+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:30:04.765485+0000 | compress | METRIC - error 3119.92\n",
      "2026-01-23T06:30:04.765934+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:30:04.766185+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:30:04.766649+0000 | compress_modules | INFO - Quantizing model.layers.6.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:30:07.256479+0000 | compress | METRIC - time 2.49s\n",
      "2026-01-23T06:30:07.257263+0000 | compress | METRIC - error 111.18\n",
      "2026-01-23T06:30:07.257719+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:30:07.257966+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(7/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.93it/s]\n",
      "(8/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:30:21.035067+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:30:21.941951+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:30:21.942664+0000 | compress | METRIC - error 4177.26\n",
      "2026-01-23T06:30:21.943094+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:21.943455+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:30:21.943873+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:30:22.812885+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:30:22.813439+0000 | compress | METRIC - error 2474.13\n",
      "2026-01-23T06:30:22.813882+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:22.814145+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:30:22.814631+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:30:23.678140+0000 | compress | METRIC - time 0.86s\n",
      "2026-01-23T06:30:23.678759+0000 | compress | METRIC - error 359.20\n",
      "2026-01-23T06:30:23.679342+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:23.679667+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:30:23.680146+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:30:24.579063+0000 | compress | METRIC - time 0.90s\n",
      "2026-01-23T06:30:24.579629+0000 | compress | METRIC - error 41.49\n",
      "2026-01-23T06:30:24.580215+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:24.580714+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:30:24.581196+0000 | compress_modules | INFO - Quantizing model.layers.7.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:30:25.543062+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:30:25.543638+0000 | compress | METRIC - error 4252.06\n",
      "2026-01-23T06:30:25.543959+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:30:25.544185+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:30:25.544714+0000 | compress_modules | INFO - Quantizing model.layers.7.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:30:26.505637+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:30:26.506196+0000 | compress | METRIC - error 3339.72\n",
      "2026-01-23T06:30:26.506708+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:30:26.506960+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:30:26.507399+0000 | compress_modules | INFO - Quantizing model.layers.7.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:30:29.002966+0000 | compress | METRIC - time 2.50s\n",
      "2026-01-23T06:30:29.003782+0000 | compress | METRIC - error 127.30\n",
      "2026-01-23T06:30:29.004281+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:30:29.004558+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(8/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.33it/s]\n",
      "(9/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:30:42.812536+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:30:43.716222+0000 | compress | METRIC - time 0.90s\n",
      "2026-01-23T06:30:43.716812+0000 | compress | METRIC - error 5234.50\n",
      "2026-01-23T06:30:43.717216+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:43.717468+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:30:43.717900+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:30:44.625583+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:30:44.626208+0000 | compress | METRIC - error 3125.55\n",
      "2026-01-23T06:30:44.626908+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:44.627166+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:30:44.627740+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:30:45.581673+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:30:45.582279+0000 | compress | METRIC - error 446.92\n",
      "2026-01-23T06:30:45.582941+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:45.583364+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:30:45.583867+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:30:46.571199+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:30:46.571830+0000 | compress | METRIC - error 56.00\n",
      "2026-01-23T06:30:46.572240+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:30:46.572490+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:30:46.572941+0000 | compress_modules | INFO - Quantizing model.layers.8.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:30:47.588481+0000 | compress | METRIC - time 1.02s\n",
      "2026-01-23T06:30:47.589047+0000 | compress | METRIC - error 4539.45\n",
      "2026-01-23T06:30:47.589523+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:30:47.589864+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:30:47.590243+0000 | compress_modules | INFO - Quantizing model.layers.8.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:30:48.600727+0000 | compress | METRIC - time 1.01s\n",
      "2026-01-23T06:30:48.601287+0000 | compress | METRIC - error 3503.79\n",
      "2026-01-23T06:30:48.601721+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:30:48.601960+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:30:48.602377+0000 | compress_modules | INFO - Quantizing model.layers.8.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:30:51.202836+0000 | compress | METRIC - time 2.60s\n",
      "2026-01-23T06:30:51.203669+0000 | compress | METRIC - error 137.97\n",
      "2026-01-23T06:30:51.204254+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:30:51.204550+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(9/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.94it/s]\n",
      "(10/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:31:04.994252+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:31:05.935270+0000 | compress | METRIC - time 0.94s\n",
      "2026-01-23T06:31:05.935886+0000 | compress | METRIC - error 5109.09\n",
      "2026-01-23T06:31:05.936334+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:05.936571+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:31:05.937130+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:31:06.886521+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:31:06.887141+0000 | compress | METRIC - error 3000.73\n",
      "2026-01-23T06:31:06.887631+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:06.887904+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:31:06.888334+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:31:07.837553+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:31:07.838089+0000 | compress | METRIC - error 552.08\n",
      "2026-01-23T06:31:07.838594+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:07.838843+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:31:07.839303+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:31:08.825414+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:31:08.826035+0000 | compress | METRIC - error 60.32\n",
      "2026-01-23T06:31:08.826586+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:08.826864+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:31:08.827368+0000 | compress_modules | INFO - Quantizing model.layers.9.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:31:09.882359+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:31:09.882976+0000 | compress | METRIC - error 4445.20\n",
      "2026-01-23T06:31:09.883469+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:31:09.883789+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:31:09.884358+0000 | compress_modules | INFO - Quantizing model.layers.9.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:31:10.889205+0000 | compress | METRIC - time 1.00s\n",
      "2026-01-23T06:31:10.889766+0000 | compress | METRIC - error 3557.80\n",
      "2026-01-23T06:31:10.890184+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:31:10.890454+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:31:10.890997+0000 | compress_modules | INFO - Quantizing model.layers.9.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:31:13.530060+0000 | compress | METRIC - time 2.64s\n",
      "2026-01-23T06:31:13.530903+0000 | compress | METRIC - error 138.06\n",
      "2026-01-23T06:31:13.531345+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:31:13.531598+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(10/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 170.46it/s]\n",
      "(11/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:31:27.317145+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:31:28.298305+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:31:28.298869+0000 | compress | METRIC - error 5113.89\n",
      "2026-01-23T06:31:28.299299+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:28.299553+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:31:28.300088+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:31:29.168715+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:31:29.169262+0000 | compress | METRIC - error 3134.33\n",
      "2026-01-23T06:31:29.169741+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:29.169983+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:31:29.170482+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:31:30.121738+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:31:30.122239+0000 | compress | METRIC - error 430.89\n",
      "2026-01-23T06:31:30.122787+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:30.123064+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:31:30.123495+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:31:31.099409+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:31:31.100029+0000 | compress | METRIC - error 55.29\n",
      "2026-01-23T06:31:31.100456+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:31.100793+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:31:31.101403+0000 | compress_modules | INFO - Quantizing model.layers.10.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:31:32.151021+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:31:32.151725+0000 | compress | METRIC - error 4418.78\n",
      "2026-01-23T06:31:32.152137+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:31:32.152374+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:31:32.152796+0000 | compress_modules | INFO - Quantizing model.layers.10.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:31:33.190500+0000 | compress | METRIC - time 1.04s\n",
      "2026-01-23T06:31:33.191078+0000 | compress | METRIC - error 3775.76\n",
      "2026-01-23T06:31:33.191426+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:31:33.191646+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:31:33.192148+0000 | compress_modules | INFO - Quantizing model.layers.10.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:31:35.699959+0000 | compress | METRIC - time 2.51s\n",
      "2026-01-23T06:31:35.700763+0000 | compress | METRIC - error 152.92\n",
      "2026-01-23T06:31:35.701269+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:31:35.701526+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(11/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 170.48it/s]\n",
      "(12/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:31:49.488595+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:31:50.437303+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:31:50.437876+0000 | compress | METRIC - error 4460.34\n",
      "2026-01-23T06:31:50.438385+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:50.438656+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:31:50.439119+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:31:51.347386+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:31:51.347978+0000 | compress | METRIC - error 2493.98\n",
      "2026-01-23T06:31:51.348481+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:51.348772+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:31:51.349210+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:31:52.261268+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:31:52.261755+0000 | compress | METRIC - error 530.24\n",
      "2026-01-23T06:31:52.262212+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:52.262456+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:31:52.262896+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:31:53.207522+0000 | compress | METRIC - time 0.94s\n",
      "2026-01-23T06:31:53.208105+0000 | compress | METRIC - error 74.74\n",
      "2026-01-23T06:31:53.208545+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:31:53.208782+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:31:53.209193+0000 | compress_modules | INFO - Quantizing model.layers.11.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:31:54.187662+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:31:54.188224+0000 | compress | METRIC - error 4576.06\n",
      "2026-01-23T06:31:54.188668+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:31:54.188901+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:31:54.189337+0000 | compress_modules | INFO - Quantizing model.layers.11.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:31:55.163161+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:31:55.163746+0000 | compress | METRIC - error 4058.17\n",
      "2026-01-23T06:31:55.164178+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:31:55.164423+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:31:55.164846+0000 | compress_modules | INFO - Quantizing model.layers.11.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:31:57.777111+0000 | compress | METRIC - time 2.61s\n",
      "2026-01-23T06:31:57.778016+0000 | compress | METRIC - error 173.35\n",
      "2026-01-23T06:31:57.778462+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:31:57.778717+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(12/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.90it/s]\n",
      "(13/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:32:11.584685+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:32:12.492606+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:32:12.493137+0000 | compress | METRIC - error 6070.62\n",
      "2026-01-23T06:32:12.493601+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:12.493866+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:32:12.494308+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:32:13.368208+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:32:13.368840+0000 | compress | METRIC - error 3595.77\n",
      "2026-01-23T06:32:13.369309+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:13.369697+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:32:13.370146+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:32:14.263173+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:32:14.263706+0000 | compress | METRIC - error 563.08\n",
      "2026-01-23T06:32:14.264110+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:14.264363+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:32:14.264797+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:32:15.210334+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:32:15.210942+0000 | compress | METRIC - error 91.77\n",
      "2026-01-23T06:32:15.211389+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:15.211656+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:32:15.212097+0000 | compress_modules | INFO - Quantizing model.layers.12.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:32:16.218804+0000 | compress | METRIC - time 1.01s\n",
      "2026-01-23T06:32:16.219417+0000 | compress | METRIC - error 4865.57\n",
      "2026-01-23T06:32:16.219871+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:32:16.220121+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:32:16.220655+0000 | compress_modules | INFO - Quantizing model.layers.12.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:32:17.202996+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:32:17.203592+0000 | compress | METRIC - error 4351.55\n",
      "2026-01-23T06:32:17.204035+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:32:17.204283+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:32:17.204722+0000 | compress_modules | INFO - Quantizing model.layers.12.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:32:19.812733+0000 | compress | METRIC - time 2.61s\n",
      "2026-01-23T06:32:19.813614+0000 | compress | METRIC - error 197.66\n",
      "2026-01-23T06:32:19.814052+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:32:19.814300+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(13/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.75it/s]\n",
      "(14/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:32:33.632586+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:32:34.540244+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:32:34.540798+0000 | compress | METRIC - error 6361.19\n",
      "2026-01-23T06:32:34.541213+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:34.541465+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:32:34.541901+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:32:35.409339+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:32:35.409829+0000 | compress | METRIC - error 4046.89\n",
      "2026-01-23T06:32:35.410272+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:35.410576+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:32:35.411094+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:32:36.345293+0000 | compress | METRIC - time 0.93s\n",
      "2026-01-23T06:32:36.345870+0000 | compress | METRIC - error 650.76\n",
      "2026-01-23T06:32:36.346380+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:36.346653+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:32:36.347170+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:32:37.369299+0000 | compress | METRIC - time 1.02s\n",
      "2026-01-23T06:32:37.369918+0000 | compress | METRIC - error 104.51\n",
      "2026-01-23T06:32:37.370409+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:37.370682+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:32:37.371175+0000 | compress_modules | INFO - Quantizing model.layers.13.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:32:38.376221+0000 | compress | METRIC - time 1.00s\n",
      "2026-01-23T06:32:38.376836+0000 | compress | METRIC - error 5714.47\n",
      "2026-01-23T06:32:38.377305+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:32:38.377568+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:32:38.378007+0000 | compress_modules | INFO - Quantizing model.layers.13.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:32:39.341051+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:32:39.341617+0000 | compress | METRIC - error 4815.38\n",
      "2026-01-23T06:32:39.342065+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:32:39.342309+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:32:39.342764+0000 | compress_modules | INFO - Quantizing model.layers.13.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:32:41.863532+0000 | compress | METRIC - time 2.52s\n",
      "2026-01-23T06:32:41.864517+0000 | compress | METRIC - error 253.23\n",
      "2026-01-23T06:32:41.864960+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:32:41.865201+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(14/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 170.02it/s]\n",
      "(15/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:32:55.674629+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:32:56.681123+0000 | compress | METRIC - time 1.01s\n",
      "2026-01-23T06:32:56.681730+0000 | compress | METRIC - error 6983.91\n",
      "2026-01-23T06:32:56.682215+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:56.682590+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:32:56.683019+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:32:57.670975+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:32:57.671552+0000 | compress | METRIC - error 3310.89\n",
      "2026-01-23T06:32:57.672006+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:57.672259+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:32:57.672841+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:32:58.588775+0000 | compress | METRIC - time 0.92s\n",
      "2026-01-23T06:32:58.589374+0000 | compress | METRIC - error 766.75\n",
      "2026-01-23T06:32:58.589813+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:58.590052+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:32:58.590514+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:32:59.561311+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:32:59.561943+0000 | compress | METRIC - error 130.15\n",
      "2026-01-23T06:32:59.562439+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:32:59.562811+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:32:59.563228+0000 | compress_modules | INFO - Quantizing model.layers.14.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:33:00.615440+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:33:00.616039+0000 | compress | METRIC - error 6256.93\n",
      "2026-01-23T06:33:00.616526+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:33:00.616791+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:33:00.617291+0000 | compress_modules | INFO - Quantizing model.layers.14.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:33:01.579576+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:33:01.580152+0000 | compress | METRIC - error 5226.71\n",
      "2026-01-23T06:33:01.580728+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:33:01.581044+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:33:01.581653+0000 | compress_modules | INFO - Quantizing model.layers.14.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:33:04.308763+0000 | compress | METRIC - time 2.73s\n",
      "2026-01-23T06:33:04.309676+0000 | compress | METRIC - error 314.39\n",
      "2026-01-23T06:33:04.310141+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:33:04.310391+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(15/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 170.61it/s]\n",
      "(16/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:33:18.106104+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:33:19.053666+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:33:19.054271+0000 | compress | METRIC - error 7261.36\n",
      "2026-01-23T06:33:19.054699+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:33:19.054934+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:33:19.055336+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:33:20.002174+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:33:20.002882+0000 | compress | METRIC - error 3772.13\n",
      "2026-01-23T06:33:20.003325+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:33:20.003559+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:33:20.003973+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:33:20.956689+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:33:20.957303+0000 | compress | METRIC - error 773.44\n",
      "2026-01-23T06:33:20.957781+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:33:20.958125+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:33:20.958536+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:33:21.862089+0000 | compress | METRIC - time 0.90s\n",
      "2026-01-23T06:33:21.862681+0000 | compress | METRIC - error 84.99\n",
      "2026-01-23T06:33:21.863115+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:33:21.863396+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:33:21.863866+0000 | compress_modules | INFO - Quantizing model.layers.15.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:33:22.876669+0000 | compress | METRIC - time 1.01s\n",
      "2026-01-23T06:33:22.877420+0000 | compress | METRIC - error 6874.94\n",
      "2026-01-23T06:33:22.877914+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:33:22.878177+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:33:22.878586+0000 | compress_modules | INFO - Quantizing model.layers.15.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:33:23.932591+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:33:23.933205+0000 | compress | METRIC - error 5335.67\n",
      "2026-01-23T06:33:23.933611+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:33:23.933843+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:33:23.934232+0000 | compress_modules | INFO - Quantizing model.layers.15.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:33:26.585049+0000 | compress | METRIC - time 2.65s\n",
      "2026-01-23T06:33:26.585899+0000 | compress | METRIC - error 335.05\n",
      "2026-01-23T06:33:26.586321+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:33:26.586634+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(16/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 170.33it/s]\n",
      "(17/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:33:40.395038+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:33:41.301080+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:33:41.301623+0000 | compress | METRIC - error 7588.73\n",
      "2026-01-23T06:33:41.302058+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:33:41.302305+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:33:41.302776+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:33:42.170176+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:33:42.170662+0000 | compress | METRIC - error 4240.04\n",
      "2026-01-23T06:33:42.171085+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:33:42.171334+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:33:42.171791+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:33:43.041409+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:33:43.042013+0000 | compress | METRIC - error 859.71\n",
      "2026-01-23T06:33:43.042487+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:33:43.042744+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:33:43.043219+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:33:43.938396+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:33:43.938943+0000 | compress | METRIC - error 60.94\n",
      "2026-01-23T06:33:43.939422+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:33:43.939678+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:33:43.940135+0000 | compress_modules | INFO - Quantizing model.layers.16.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:33:44.906497+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:33:44.907088+0000 | compress | METRIC - error 7183.00\n",
      "2026-01-23T06:33:44.907547+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:33:44.907802+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:33:44.908260+0000 | compress_modules | INFO - Quantizing model.layers.16.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:33:45.874433+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:33:45.874995+0000 | compress | METRIC - error 5417.14\n",
      "2026-01-23T06:33:45.875459+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:33:45.875894+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:33:45.876453+0000 | compress_modules | INFO - Quantizing model.layers.16.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:33:48.390362+0000 | compress | METRIC - time 2.51s\n",
      "2026-01-23T06:33:48.391177+0000 | compress | METRIC - error 324.47\n",
      "2026-01-23T06:33:48.391700+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:33:48.391940+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(17/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 170.30it/s]\n",
      "(18/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:34:02.205672+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:34:03.114473+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:34:03.115042+0000 | compress | METRIC - error 7360.72\n",
      "2026-01-23T06:34:03.115573+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:03.115871+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:34:03.116318+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:34:04.111544+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:34:04.112137+0000 | compress | METRIC - error 3976.41\n",
      "2026-01-23T06:34:04.112669+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:04.113043+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:34:04.113481+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:34:05.073687+0000 | compress | METRIC - time 0.96s\n",
      "2026-01-23T06:34:05.074292+0000 | compress | METRIC - error 841.28\n",
      "2026-01-23T06:34:05.074765+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:05.075033+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:34:05.075524+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:34:05.965305+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:34:05.965852+0000 | compress | METRIC - error 60.11\n",
      "2026-01-23T06:34:05.966297+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:05.966562+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:34:05.966989+0000 | compress_modules | INFO - Quantizing model.layers.17.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:34:06.950184+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:34:06.950733+0000 | compress | METRIC - error 7664.76\n",
      "2026-01-23T06:34:06.951184+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:34:06.951495+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:34:06.951982+0000 | compress_modules | INFO - Quantizing model.layers.17.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:34:07.922681+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:34:07.923242+0000 | compress | METRIC - error 5682.24\n",
      "2026-01-23T06:34:07.923739+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:34:07.924105+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:34:07.924520+0000 | compress_modules | INFO - Quantizing model.layers.17.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:34:10.509070+0000 | compress | METRIC - time 2.58s\n",
      "2026-01-23T06:34:10.509931+0000 | compress | METRIC - error 350.36\n",
      "2026-01-23T06:34:10.510425+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:34:10.510890+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(18/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.37it/s]\n",
      "(19/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:34:24.350410+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:34:25.270986+0000 | compress | METRIC - time 0.92s\n",
      "2026-01-23T06:34:25.271543+0000 | compress | METRIC - error 8071.42\n",
      "2026-01-23T06:34:25.271960+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:25.272179+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:34:25.272684+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:34:26.144777+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:34:26.145431+0000 | compress | METRIC - error 4455.64\n",
      "2026-01-23T06:34:26.146031+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:26.146323+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:34:26.146962+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:34:27.042392+0000 | compress | METRIC - time 0.90s\n",
      "2026-01-23T06:34:27.042916+0000 | compress | METRIC - error 1026.81\n",
      "2026-01-23T06:34:27.043352+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:27.043619+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:34:27.044083+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:34:27.987379+0000 | compress | METRIC - time 0.94s\n",
      "2026-01-23T06:34:27.988005+0000 | compress | METRIC - error 68.59\n",
      "2026-01-23T06:34:27.988539+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:27.988787+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:34:27.989182+0000 | compress_modules | INFO - Quantizing model.layers.18.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:34:28.959320+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:34:28.959901+0000 | compress | METRIC - error 8164.10\n",
      "2026-01-23T06:34:28.960322+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:34:28.960567+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:34:28.961065+0000 | compress_modules | INFO - Quantizing model.layers.18.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:34:29.933239+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:34:29.933844+0000 | compress | METRIC - error 6180.44\n",
      "2026-01-23T06:34:29.934279+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:34:29.934539+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:34:29.935033+0000 | compress_modules | INFO - Quantizing model.layers.18.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:34:32.521371+0000 | compress | METRIC - time 2.59s\n",
      "2026-01-23T06:34:32.522276+0000 | compress | METRIC - error 386.90\n",
      "2026-01-23T06:34:32.522784+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:34:32.523054+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(19/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.61it/s]\n",
      "(20/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:34:46.368254+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:34:47.352659+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:34:47.353254+0000 | compress | METRIC - error 7542.83\n",
      "2026-01-23T06:34:47.353927+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:47.354216+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:34:47.354756+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:34:48.295142+0000 | compress | METRIC - time 0.94s\n",
      "2026-01-23T06:34:48.295792+0000 | compress | METRIC - error 4344.13\n",
      "2026-01-23T06:34:48.296207+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:48.296457+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:34:48.296915+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:34:49.162292+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:34:49.162855+0000 | compress | METRIC - error 1053.46\n",
      "2026-01-23T06:34:49.163278+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:49.163591+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:34:49.163974+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:34:50.077627+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:34:50.078364+0000 | compress | METRIC - error 91.86\n",
      "2026-01-23T06:34:50.078767+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:34:50.078991+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:34:50.079392+0000 | compress_modules | INFO - Quantizing model.layers.19.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:34:51.057885+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:34:51.058484+0000 | compress | METRIC - error 8700.62\n",
      "2026-01-23T06:34:51.058867+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:34:51.059100+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:34:51.059515+0000 | compress_modules | INFO - Quantizing model.layers.19.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:34:52.032803+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:34:52.033450+0000 | compress | METRIC - error 6685.36\n",
      "2026-01-23T06:34:52.033878+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:34:52.034099+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:34:52.034506+0000 | compress_modules | INFO - Quantizing model.layers.19.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:34:54.761144+0000 | compress | METRIC - time 2.73s\n",
      "2026-01-23T06:34:54.762099+0000 | compress | METRIC - error 464.40\n",
      "2026-01-23T06:34:54.762526+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:34:54.762789+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(20/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.32it/s]\n",
      "(21/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:35:08.597887+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:35:09.545906+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:35:09.546467+0000 | compress | METRIC - error 7857.23\n",
      "2026-01-23T06:35:09.546919+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:09.547174+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:35:09.547653+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:35:10.456211+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:35:10.456782+0000 | compress | METRIC - error 4696.80\n",
      "2026-01-23T06:35:10.457225+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:10.457487+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:35:10.457955+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:35:11.367084+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:35:11.367645+0000 | compress | METRIC - error 1266.66\n",
      "2026-01-23T06:35:11.367973+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:11.368191+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:35:11.368586+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:35:12.353390+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:35:12.354115+0000 | compress | METRIC - error 76.78\n",
      "2026-01-23T06:35:12.354601+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:12.354833+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:35:12.355230+0000 | compress_modules | INFO - Quantizing model.layers.20.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:35:13.425394+0000 | compress | METRIC - time 1.07s\n",
      "2026-01-23T06:35:13.426020+0000 | compress | METRIC - error 8592.85\n",
      "2026-01-23T06:35:13.426607+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:35:13.426904+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:35:13.427397+0000 | compress_modules | INFO - Quantizing model.layers.20.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:35:14.470240+0000 | compress | METRIC - time 1.04s\n",
      "2026-01-23T06:35:14.470849+0000 | compress | METRIC - error 6903.70\n",
      "2026-01-23T06:35:14.471190+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:35:14.471469+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:35:14.472011+0000 | compress_modules | INFO - Quantizing model.layers.20.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:35:17.040374+0000 | compress | METRIC - time 2.57s\n",
      "2026-01-23T06:35:17.041307+0000 | compress | METRIC - error 468.37\n",
      "2026-01-23T06:35:17.041870+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:35:17.042286+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(21/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.80it/s]\n",
      "(22/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:35:30.869084+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:35:31.789492+0000 | compress | METRIC - time 0.92s\n",
      "2026-01-23T06:35:31.790033+0000 | compress | METRIC - error 7752.72\n",
      "2026-01-23T06:35:31.790492+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:31.790766+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:35:31.791217+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:35:32.740740+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:35:32.741462+0000 | compress | METRIC - error 4578.25\n",
      "2026-01-23T06:35:32.741987+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:32.742352+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:35:32.742777+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:35:33.691040+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:35:33.691592+0000 | compress | METRIC - error 1635.25\n",
      "2026-01-23T06:35:33.692052+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:33.692296+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:35:33.692744+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:35:34.605340+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:35:34.605978+0000 | compress | METRIC - error 88.50\n",
      "2026-01-23T06:35:34.606455+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:34.606785+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:35:34.607245+0000 | compress_modules | INFO - Quantizing model.layers.21.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:35:35.577161+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:35:35.577721+0000 | compress | METRIC - error 9337.83\n",
      "2026-01-23T06:35:35.578178+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:35:35.578421+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:35:35.578908+0000 | compress_modules | INFO - Quantizing model.layers.21.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:35:36.548460+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:35:36.549044+0000 | compress | METRIC - error 7440.41\n",
      "2026-01-23T06:35:36.549492+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:35:36.549748+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:35:36.550165+0000 | compress_modules | INFO - Quantizing model.layers.21.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:35:39.297397+0000 | compress | METRIC - time 2.75s\n",
      "2026-01-23T06:35:39.298298+0000 | compress | METRIC - error 500.13\n",
      "2026-01-23T06:35:39.298740+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:35:39.298988+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(22/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.20it/s]\n",
      "(23/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:35:53.132664+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:35:54.081266+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:35:54.081813+0000 | compress | METRIC - error 7924.70\n",
      "2026-01-23T06:35:54.082216+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:54.082598+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:35:54.082991+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:35:54.997685+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:35:54.998289+0000 | compress | METRIC - error 4512.09\n",
      "2026-01-23T06:35:54.998723+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:54.999037+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:35:54.999409+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:35:55.913652+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:35:55.914207+0000 | compress | METRIC - error 1693.76\n",
      "2026-01-23T06:35:55.914683+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:55.914942+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:35:55.915488+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:35:56.852679+0000 | compress | METRIC - time 0.94s\n",
      "2026-01-23T06:35:56.853252+0000 | compress | METRIC - error 70.81\n",
      "2026-01-23T06:35:56.853696+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:35:56.853938+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:35:56.854356+0000 | compress_modules | INFO - Quantizing model.layers.22.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:35:57.863824+0000 | compress | METRIC - time 1.01s\n",
      "2026-01-23T06:35:57.864429+0000 | compress | METRIC - error 10121.54\n",
      "2026-01-23T06:35:57.864839+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:35:57.865074+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:35:57.865473+0000 | compress_modules | INFO - Quantizing model.layers.22.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:35:58.919987+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:35:58.920612+0000 | compress | METRIC - error 8032.20\n",
      "2026-01-23T06:35:58.921092+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:35:58.921378+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:35:58.921836+0000 | compress_modules | INFO - Quantizing model.layers.22.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:36:01.501261+0000 | compress | METRIC - time 2.58s\n",
      "2026-01-23T06:36:01.502229+0000 | compress | METRIC - error 561.57\n",
      "2026-01-23T06:36:01.502685+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:36:01.502947+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(23/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 168.69it/s]\n",
      "(24/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:36:15.346299+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:36:16.256139+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:36:16.256715+0000 | compress | METRIC - error 7725.31\n",
      "2026-01-23T06:36:16.257144+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:36:16.257400+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:36:16.257838+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:36:17.129875+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:36:17.130391+0000 | compress | METRIC - error 4723.49\n",
      "2026-01-23T06:36:17.130849+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:36:17.131108+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:36:17.131592+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:36:18.002368+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:36:18.002930+0000 | compress | METRIC - error 1637.22\n",
      "2026-01-23T06:36:18.003497+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:36:18.003800+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:36:18.004274+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:36:18.914940+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:36:18.915533+0000 | compress | METRIC - error 111.33\n",
      "2026-01-23T06:36:18.916039+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:36:18.916401+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:36:18.916876+0000 | compress_modules | INFO - Quantizing model.layers.23.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:36:19.968428+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:36:19.969090+0000 | compress | METRIC - error 11526.76\n",
      "2026-01-23T06:36:19.969571+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:36:19.969885+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:36:19.970235+0000 | compress_modules | INFO - Quantizing model.layers.23.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:36:21.041245+0000 | compress | METRIC - time 1.07s\n",
      "2026-01-23T06:36:21.041835+0000 | compress | METRIC - error 8747.33\n",
      "2026-01-23T06:36:21.042291+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:36:21.042582+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:36:21.043050+0000 | compress_modules | INFO - Quantizing model.layers.23.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:36:23.638059+0000 | compress | METRIC - time 2.59s\n",
      "2026-01-23T06:36:23.639322+0000 | compress | METRIC - error 650.32\n",
      "2026-01-23T06:36:23.639770+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:36:23.640026+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(24/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.00it/s]\n",
      "(25/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:36:37.477509+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:36:38.385870+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:36:38.386453+0000 | compress | METRIC - error 8377.51\n",
      "2026-01-23T06:36:38.386904+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:36:38.387147+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:36:38.387569+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:36:39.272581+0000 | compress | METRIC - time 0.88s\n",
      "2026-01-23T06:36:39.273118+0000 | compress | METRIC - error 5162.77\n",
      "2026-01-23T06:36:39.273552+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:36:39.273867+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:36:39.274239+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:36:40.180562+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:36:40.181200+0000 | compress | METRIC - error 2331.51\n",
      "2026-01-23T06:36:40.181640+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:36:40.181887+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:36:40.182302+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:36:41.126611+0000 | compress | METRIC - time 0.94s\n",
      "2026-01-23T06:36:41.127196+0000 | compress | METRIC - error 163.39\n",
      "2026-01-23T06:36:41.127696+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:36:41.128003+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:36:41.128503+0000 | compress_modules | INFO - Quantizing model.layers.24.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:36:42.116658+0000 | compress | METRIC - time 0.99s\n",
      "2026-01-23T06:36:42.117276+0000 | compress | METRIC - error 13130.95\n",
      "2026-01-23T06:36:42.117707+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:36:42.117949+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:36:42.118367+0000 | compress_modules | INFO - Quantizing model.layers.24.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:36:43.098000+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:36:43.098597+0000 | compress | METRIC - error 9810.08\n",
      "2026-01-23T06:36:43.099034+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:36:43.099280+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:36:43.099746+0000 | compress_modules | INFO - Quantizing model.layers.24.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:36:45.656398+0000 | compress | METRIC - time 2.56s\n",
      "2026-01-23T06:36:45.657202+0000 | compress | METRIC - error 775.97\n",
      "2026-01-23T06:36:45.657644+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:36:45.657888+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(25/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 168.47it/s]\n",
      "(26/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:36:59.511915+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:37:00.420959+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:37:00.421611+0000 | compress | METRIC - error 8701.06\n",
      "2026-01-23T06:37:00.422020+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:00.422261+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:37:00.422719+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:37:01.287776+0000 | compress | METRIC - time 0.86s\n",
      "2026-01-23T06:37:01.288296+0000 | compress | METRIC - error 4485.78\n",
      "2026-01-23T06:37:01.288742+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:01.288977+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:37:01.289397+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:37:02.160302+0000 | compress | METRIC - time 0.87s\n",
      "2026-01-23T06:37:02.161060+0000 | compress | METRIC - error 2226.64\n",
      "2026-01-23T06:37:02.161498+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:02.161738+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:37:02.162175+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:37:03.065891+0000 | compress | METRIC - time 0.90s\n",
      "2026-01-23T06:37:03.066478+0000 | compress | METRIC - error 142.34\n",
      "2026-01-23T06:37:03.066927+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:03.067171+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:37:03.067605+0000 | compress_modules | INFO - Quantizing model.layers.25.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:37:04.037857+0000 | compress | METRIC - time 0.97s\n",
      "2026-01-23T06:37:04.038482+0000 | compress | METRIC - error 14190.17\n",
      "2026-01-23T06:37:04.038899+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:37:04.039136+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:37:04.039574+0000 | compress_modules | INFO - Quantizing model.layers.25.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:37:05.090308+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:37:05.090957+0000 | compress | METRIC - error 10616.38\n",
      "2026-01-23T06:37:05.091445+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:37:05.091747+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:37:05.092196+0000 | compress_modules | INFO - Quantizing model.layers.25.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:37:07.684448+0000 | compress | METRIC - time 2.59s\n",
      "2026-01-23T06:37:07.685325+0000 | compress | METRIC - error 968.97\n",
      "2026-01-23T06:37:07.685760+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:37:07.685996+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(26/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 169.33it/s]\n",
      "(27/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:37:21.528246+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:37:22.480015+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:37:22.480644+0000 | compress | METRIC - error 7487.92\n",
      "2026-01-23T06:37:22.481104+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:22.481444+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:37:22.481849+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:37:23.376929+0000 | compress | METRIC - time 0.89s\n",
      "2026-01-23T06:37:23.377567+0000 | compress | METRIC - error 4801.38\n",
      "2026-01-23T06:37:23.378003+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:23.378255+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:37:23.378724+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:37:24.326907+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:37:24.327564+0000 | compress | METRIC - error 2863.52\n",
      "2026-01-23T06:37:24.328012+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:24.328276+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:37:24.328726+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:37:25.325484+0000 | compress | METRIC - time 1.00s\n",
      "2026-01-23T06:37:25.326113+0000 | compress | METRIC - error 292.76\n",
      "2026-01-23T06:37:25.326544+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:25.326812+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:37:25.327227+0000 | compress_modules | INFO - Quantizing model.layers.26.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:37:26.375302+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:37:26.375955+0000 | compress | METRIC - error 14938.59\n",
      "2026-01-23T06:37:26.376434+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:37:26.376712+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:37:26.377165+0000 | compress_modules | INFO - Quantizing model.layers.26.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:37:27.404764+0000 | compress | METRIC - time 1.03s\n",
      "2026-01-23T06:37:27.405362+0000 | compress | METRIC - error 11047.92\n",
      "2026-01-23T06:37:27.405937+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:37:27.406234+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:37:27.406888+0000 | compress_modules | INFO - Quantizing model.layers.26.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:37:29.964623+0000 | compress | METRIC - time 2.56s\n",
      "2026-01-23T06:37:29.965562+0000 | compress | METRIC - error 1288.94\n",
      "2026-01-23T06:37:29.966148+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:37:29.966467+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(27/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 170.03it/s]\n",
      "(28/29): Calibrating: 100%|██████████| 512/512 [00:10<00:00, 47.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:37:43.795586+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.q_proj using 512 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:37:44.706367+0000 | compress | METRIC - time 0.91s\n",
      "2026-01-23T06:37:44.706972+0000 | compress | METRIC - error 6126.65\n",
      "2026-01-23T06:37:44.707407+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:44.707677+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:37:44.708105+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.k_proj using 512 samples\n",
      "2026-01-23T06:37:45.661212+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:37:45.661841+0000 | compress | METRIC - error 3450.79\n",
      "2026-01-23T06:37:45.662323+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:45.662659+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:37:45.663247+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.v_proj using 512 samples\n",
      "2026-01-23T06:37:46.612705+0000 | compress | METRIC - time 0.95s\n",
      "2026-01-23T06:37:46.613397+0000 | compress | METRIC - error 1895.07\n",
      "2026-01-23T06:37:46.613772+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:46.613990+0000 | compress | METRIC - Compressed module size: 6.365184 MB\n",
      "2026-01-23T06:37:46.614454+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.o_proj using 512 samples\n",
      "2026-01-23T06:37:47.598360+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:37:47.599104+0000 | compress | METRIC - error 649.41\n",
      "2026-01-23T06:37:47.599442+0000 | compress | METRIC - GPU 0 | usage: 12.24% | total memory: 25 GB\n",
      "2026-01-23T06:37:47.599687+0000 | compress | METRIC - Compressed module size: 19.095552 MB\n",
      "2026-01-23T06:37:47.600086+0000 | compress_modules | INFO - Quantizing model.layers.27.mlp.gate_proj using 512 samples\n",
      "2026-01-23T06:37:48.650658+0000 | compress | METRIC - time 1.05s\n",
      "2026-01-23T06:37:48.651235+0000 | compress | METRIC - error 13985.18\n",
      "2026-01-23T06:37:48.651672+0000 | compress | METRIC - GPU 0 | usage: 12.25% | total memory: 25 GB\n",
      "2026-01-23T06:37:48.651914+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:37:48.652334+0000 | compress_modules | INFO - Quantizing model.layers.27.mlp.up_proj using 512 samples\n",
      "2026-01-23T06:37:49.633867+0000 | compress | METRIC - time 0.98s\n",
      "2026-01-23T06:37:49.634458+0000 | compress | METRIC - error 11660.71\n",
      "2026-01-23T06:37:49.634874+0000 | compress | METRIC - GPU 0 | usage: 12.26% | total memory: 25 GB\n",
      "2026-01-23T06:37:49.635111+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n",
      "2026-01-23T06:37:49.635525+0000 | compress_modules | INFO - Quantizing model.layers.27.mlp.down_proj using 512 samples\n",
      "2026-01-23T06:37:52.247613+0000 | compress | METRIC - time 2.61s\n",
      "2026-01-23T06:37:52.248531+0000 | compress | METRIC - error 2853.07\n",
      "2026-01-23T06:37:52.248967+0000 | compress | METRIC - GPU 0 | usage: 13.30% | total memory: 25 GB\n",
      "2026-01-23T06:37:52.249217+0000 | compress | METRIC - Compressed module size: 50.921472 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(28/29): Propagating: 100%|██████████| 512/512 [00:03<00:00, 168.96it/s]\n",
      "(29/29): Calibrating: 100%|██████████| 512/512 [00:00<00:00, 1279.18it/s]\n",
      "(29/29): Propagating: 100%|██████████| 512/512 [00:00<00:00, 1444.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:37:56.158784+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23T06:37:56.188614+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 196it [00:05, 36.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply quantization with GPTQ recipe\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "\n",
    "# Configure the quantization algorithm to run.\n",
    "recipe = GPTQModifier(targets=\"Linear\", scheme=\"W4A16\", ignore=[\"lm_head\"])\n",
    "\n",
    "# Save dir name\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-W4A16-G128\"\n",
    "\n",
    "# Apply quantization.\n",
    "oneshot(\n",
    "    model=model, dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    output_dir=SAVE_DIR,    # Tokenizer automatically saved together\n",
    ")\n",
    "\n",
    "# # Save to disk compressed.\n",
    "# model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "# tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de8976",
   "metadata": {},
   "source": [
    "결과: 6.43Gb -> 2.82Gb로 약 44% 압축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca07c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need.\n",
    "!pip install -U lm_eval       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e5c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-23 06:38:44] WARNING __main__.py:369:  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "[2026-01-23 06:38:44] INFO __main__.py:465: Selected Tasks: ['gsm8k']\n",
      "[2026-01-23 06:38:44] WARNING evaluator.py:172: pretrained=pretrained=./Llama-3.2-3B-Instruct-W4A16-G128,add_bos_token=true appears to be an instruct or chat variant but chat template is\n",
      "        not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "[2026-01-23 06:38:44] INFO evaluator.py:202: Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "[2026-01-23 06:38:44] INFO evaluator.py:240: Initializing vllm model, with arguments: {'pretrained': './Llama-3.2-3B-Instruct-W4A16-G128', 'add_bos_token': True}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:44\u001b[0m \u001b[90m[utils.py:263]\u001b[0m non-default args: {'seed': 1234, 'disable_log_stats': True, 'model': './Llama-3.2-3B-Instruct-W4A16-G128'}\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:44\u001b[0m \u001b[90m[model.py:530]\u001b[0m Resolved architecture: LlamaForCausalLM\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:44\u001b[0m \u001b[90m[model.py:1545]\u001b[0m Using max model len 131072\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:45\u001b[0m \u001b[90m[scheduler.py:229]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:45\u001b[0m \u001b[90m[vllm.py:630]\u001b[0m Asynchronous scheduling is enabled.\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:45\u001b[0m \u001b[90m[vllm.py:637]\u001b[0m Disabling NCCL for DP synchronization when using async scheduling.\n",
      "The tokenizer you are loading from './Llama-3.2-3B-Instruct-W4A16-G128' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:46\u001b[0m \u001b[90m[core.py:97]\u001b[0m Initializing a V1 LLM engine (v0.14.0) with config: model='./Llama-3.2-3B-Instruct-W4A16-G128', speculative_config=None, tokenizer='./Llama-3.2-3B-Instruct-W4A16-G128', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=1234, served_model_name=./Llama-3.2-3B-Instruct-W4A16-G128, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:46\u001b[0m \u001b[90m[parallel_state.py:1214]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.3:47435 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:46\u001b[0m \u001b[90m[parallel_state.py:1425]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:47\u001b[0m \u001b[90m[gpu_model_runner.py:3808]\u001b[0m Starting to load model ./Llama-3.2-3B-Instruct-W4A16-G128...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:47\u001b[0m \u001b[90m[compressed_tensors_wNa16.py:114]\u001b[0m Using MarlinLinearKernel for CompressedTensorsWNA16\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:47\u001b[0m \u001b[90m[cuda.py:351]\u001b[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:48\u001b[0m \u001b[90m[default_loader.py:291]\u001b[0m Loading weights took 0.36 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:48\u001b[0m \u001b[90m[gpu_model_runner.py:3905]\u001b[0m Model loading took 2.13 GiB memory and 0.722318 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:54\u001b[0m \u001b[90m[backends.py:644]\u001b[0m Using cache directory: /root/.cache/vllm/torch_compile_cache/9b96f33526/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:54\u001b[0m \u001b[90m[backends.py:704]\u001b[0m Dynamo bytecode transform time: 5.30 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:57\u001b[0m \u001b[90m[backends.py:226]\u001b[0m Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.754 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:57\u001b[0m \u001b[90m[monitor.py:34]\u001b[0m torch.compile takes 6.05 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:58\u001b[0m \u001b[90m[gpu_worker.py:358]\u001b[0m Available KV cache memory: 17.83 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:58\u001b[0m \u001b[90m[kv_cache_utils.py:1305]\u001b[0m GPU KV cache size: 166,928 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:38:58\u001b[0m \u001b[90m[kv_cache_utils.py:1310]\u001b[0m Maximum concurrency for 131,072 tokens per request: 1.27x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 51/51 [00:01<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:01<00:00, 34.34it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:39:01\u001b[0m \u001b[90m[gpu_model_runner.py:4856]\u001b[0m Graph capturing finished in 3 secs, took 0.50 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=208892)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-23 06:39:01\u001b[0m \u001b[90m[core.py:273]\u001b[0m init engine (profile, create kv cache, warmup model) took 13.13 seconds\n",
      "\u001b[32mINFO\u001b[0m \u001b[90m01-23 06:39:02\u001b[0m \u001b[90m[llm.py:347]\u001b[0m Supported tasks: ['generate']\n",
      "The tokenizer you are loading from './Llama-3.2-3B-Instruct-W4A16-G128' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "The tokenizer you are loading from './Llama-3.2-3B-Instruct-W4A16-G128' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "[2026-01-23 06:39:08] INFO __init__.py:695: Selected tasks:\n",
      "[2026-01-23 06:39:08] INFO __init__.py:686: Task: gsm8k (gsm8k/gsm8k.yaml)\n",
      "[2026-01-23 06:39:08] INFO evaluator.py:305: gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}\n",
      "[2026-01-23 06:39:08] WARNING evaluator.py:324: Overwriting default num_fewshot of gsm8k from 5 to 5\n",
      "[2026-01-23 06:39:08] INFO task.py:434: Building contexts for gsm8k on rank 0...\n",
      "100%|████████████████████████████████████████| 250/250 [00:00<00:00, 457.51it/s]\n",
      "[2026-01-23 06:39:09] INFO evaluator.py:574: Running generate_until requests\n",
      "Running generate_until requests:   0%|                  | 0/250 [00:00<?, ?it/s]\n",
      "Adding requests: 100%|██████████████████████| 250/250 [00:00<00:00, 7787.01it/s]\u001b[A\n",
      "\n",
      "Processed prompts:   0%| | 0/250 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,\u001b[A\n",
      "Processed prompts:   0%| | 1/250 [00:15<1:05:04, 15.68s/it, est. speed input: 71\u001b[A\n",
      "Processed prompts:   1%| | 2/250 [00:16<29:11,  7.06s/it, est. speed input: 134.\u001b[A\n",
      "Processed prompts:   2%| | 5/250 [00:17<08:53,  2.18s/it, est. speed input: 302.\u001b[A\n",
      "Processed prompts:   3%| | 8/250 [00:18<04:46,  1.19s/it, est. speed input: 478.\u001b[A\n",
      "Processed prompts:   4%| | 11/250 [00:18<02:51,  1.39it/s, est. speed input: 628\u001b[A\n",
      "Processed prompts:   6%| | 15/250 [00:18<01:47,  2.18it/s, est. speed input: 811\u001b[A\n",
      "Processed prompts:   7%| | 17/250 [00:19<01:31,  2.55it/s, est. speed input: 891\u001b[A\n",
      "Processed prompts:   7%| | 18/250 [00:19<01:25,  2.70it/s, est. speed input: 934\u001b[A\n",
      "Processed prompts:  10%| | 25/250 [00:19<00:40,  5.54it/s, est. speed input: 127\u001b[A\n",
      "Processed prompts:  11%| | 27/250 [00:20<00:39,  5.69it/s, est. speed input: 136\u001b[A\n",
      "Processed prompts:  11%| | 28/250 [00:20<00:51,  4.34it/s, est. speed input: 136\u001b[A\n",
      "Processed prompts:  12%| | 30/250 [00:21<00:48,  4.52it/s, est. speed input: 143\u001b[A\n",
      "Processed prompts:  13%|▏| 32/250 [00:21<00:40,  5.41it/s, est. speed input: 150\u001b[A\n",
      "Processed prompts:  14%|▏| 35/250 [00:21<00:32,  6.59it/s, est. speed input: 160\u001b[A\n",
      "Processed prompts:  14%|▏| 36/250 [00:21<00:39,  5.44it/s, est. speed input: 162\u001b[A\n",
      "Processed prompts:  16%|▏| 39/250 [00:22<00:29,  7.07it/s, est. speed input: 172\u001b[A\n",
      "Processed prompts:  16%|▏| 40/250 [00:22<00:32,  6.39it/s, est. speed input: 174\u001b[A\n",
      "Processed prompts:  17%|▏| 43/250 [00:22<00:28,  7.22it/s, est. speed input: 184\u001b[A\n",
      "Processed prompts:  19%|▏| 47/250 [00:23<00:22,  8.98it/s, est. speed input: 200\u001b[A\n",
      "Processed prompts:  20%|▏| 49/250 [00:23<00:24,  8.31it/s, est. speed input: 205\u001b[A\n",
      "Processed prompts:  20%|▏| 50/250 [00:23<00:39,  5.06it/s, est. speed input: 204\u001b[A\n",
      "Processed prompts:  22%|▏| 54/250 [00:24<00:25,  7.56it/s, est. speed input: 218\u001b[A\n",
      "Processed prompts:  22%|▏| 56/250 [00:24<00:27,  6.96it/s, est. speed input: 223\u001b[A\n",
      "Processed prompts:  25%|▏| 62/250 [00:25<00:25,  7.45it/s, est. speed input: 240\u001b[A\n",
      "Processed prompts:  25%|▎| 63/250 [00:25<00:29,  6.41it/s, est. speed input: 241\u001b[A\n",
      "Processed prompts:  26%|▎| 66/250 [00:25<00:22,  8.10it/s, est. speed input: 250\u001b[A\n",
      "Processed prompts:  28%|▎| 69/250 [00:25<00:18,  9.83it/s, est. speed input: 260\u001b[A\n",
      "Processed prompts:  28%|▎| 71/250 [00:26<00:19,  9.04it/s, est. speed input: 264\u001b[A\n",
      "Processed prompts:  30%|▎| 74/250 [00:26<00:20,  8.52it/s, est. speed input: 271\u001b[A\n",
      "Processed prompts:  30%|▎| 76/250 [00:26<00:21,  7.99it/s, est. speed input: 274\u001b[A\n",
      "Processed prompts:  33%|▎| 82/250 [00:27<00:12, 13.63it/s, est. speed input: 296\u001b[A\n",
      "Processed prompts:  36%|▎| 90/250 [00:27<00:07, 20.70it/s, est. speed input: 321\u001b[A\n",
      "Processed prompts:  38%|▍| 96/250 [00:27<00:05, 25.68it/s, est. speed input: 339\u001b[A\n",
      "Processed prompts:  41%|▍| 102/250 [00:27<00:04, 30.41it/s, est. speed input: 35\u001b[A\n",
      "Processed prompts:  42%|▍| 106/250 [00:27<00:04, 31.02it/s, est. speed input: 36\u001b[A\n",
      "Processed prompts:  46%|▍| 114/250 [00:27<00:03, 39.79it/s, est. speed input: 39\u001b[A\n",
      "Processed prompts:  48%|▍| 121/250 [00:27<00:02, 45.28it/s, est. speed input: 41\u001b[A\n",
      "Processed prompts:  51%|▌| 127/250 [00:28<00:03, 39.13it/s, est. speed input: 43\u001b[A\n",
      "Processed prompts:  53%|▌| 132/250 [00:28<00:03, 36.58it/s, est. speed input: 44\u001b[A\n",
      "Processed prompts:  57%|▌| 143/250 [00:28<00:02, 46.16it/s, est. speed input: 47\u001b[A\n",
      "Processed prompts:  59%|▌| 148/250 [00:28<00:02, 42.45it/s, est. speed input: 48\u001b[A\n",
      "Processed prompts:  62%|▌| 155/250 [00:28<00:02, 46.68it/s, est. speed input: 50\u001b[A\n",
      "Processed prompts:  64%|▋| 160/250 [00:28<00:02, 41.79it/s, est. speed input: 51\u001b[A\n",
      "Processed prompts:  66%|▋| 166/250 [00:28<00:01, 45.41it/s, est. speed input: 53\u001b[A\n",
      "Processed prompts:  70%|▋| 175/250 [00:29<00:01, 50.38it/s, est. speed input: 55\u001b[A\n",
      "Processed prompts:  72%|▋| 181/250 [00:29<00:01, 51.03it/s, est. speed input: 57\u001b[A\n",
      "Processed prompts:  75%|▊| 188/250 [00:29<00:01, 54.83it/s, est. speed input: 58\u001b[A\n",
      "Processed prompts:  78%|▊| 194/250 [00:29<00:01, 55.04it/s, est. speed input: 60\u001b[A\n",
      "Processed prompts:  81%|▊| 202/250 [00:29<00:00, 58.81it/s, est. speed input: 62\u001b[A\n",
      "Processed prompts:  85%|▊| 213/250 [00:29<00:00, 69.89it/s, est. speed input: 65\u001b[A\n",
      "Processed prompts:  88%|▉| 221/250 [00:29<00:00, 63.74it/s, est. speed input: 67\u001b[A\n",
      "Processed prompts:  91%|▉| 228/250 [00:29<00:00, 61.71it/s, est. speed input: 68\u001b[A\n",
      "Processed prompts:  94%|▉| 235/250 [00:30<00:00, 49.46it/s, est. speed input: 70\u001b[A\n",
      "Processed prompts:  96%|▉| 241/250 [00:30<00:00, 48.43it/s, est. speed input: 71\u001b[A\n",
      "Processed prompts:  99%|▉| 247/250 [00:30<00:00, 37.64it/s, est. speed input: 72\u001b[A\n",
      "Processed prompts: 100%|█| 250/250 [00:30<00:00,  8.15it/s, est. speed input: 72\u001b[A\n",
      "Running generate_until requests: 100%|████████| 250/250 [00:30<00:00,  8.15it/s]\n",
      "[2026-01-23 06:39:43] INFO evaluation_tracker.py:280: Output path not provided, skipping saving results aggregated\n",
      "vllm (pretrained=./Llama-3.2-3B-Instruct-W4A16-G128,add_bos_token=true), gen_kwargs: (None), limit: 250.0, num_fewshot: 5, batch_size: auto\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.664|±  |0.0299|\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.556|±  |0.0315|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# our Quantized model eval\n",
    "!lm_eval --model vllm \\\n",
    "  --model_args pretrained=\"./Llama-3.2-3B-Instruct-W4A16-G128\",add_bos_token=true \\\n",
    "  --tasks gsm8k \\\n",
    "  --num_fewshot 5 \\\n",
    "  --limit 250 \\\n",
    "  --batch_size 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f494b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original model eval\n",
    "!lm_eval --model vllm \\\n",
    "  --model_args pretrained=\"meta-llama/Llama-3.2-3B-Instruct\",add_bos_token=true,dtype=auto,max_model_len=4096 \\\n",
    "  --tasks gsm8k \\\n",
    "  --num_fewshot 5 \\\n",
    "  --limit 250 \\\n",
    "  --batch_size 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada685f",
   "metadata": {},
   "source": [
    "**Quantized Model**\n",
    "\n",
    "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
    "|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
    "|gsm8k|      3|flexible-extract|     5|exact_match|↑  | 0.64|±  |0.0304|\n",
    "|     |       |strict-match    |     5|exact_match|↑  | 0.54|±  |0.0316|\n",
    "\n",
    "**Original Model**\n",
    "\n",
    "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
    "|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
    "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.680|±  |0.0296|\n",
    "|     |       |strict-match    |     5|exact_match|↑  |0.608|±  |0.0309|\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92d6f0",
   "metadata": {},
   "source": [
    "## 2.2. Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9767f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:4 sparse pruning with/without FP8 quantization\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.obcq import SparseGPTModifier\n",
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor.utils import dispatch_for_generation\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "DATASET_SPLIT = \"train_sft\"\n",
    "NUM_CALIBRATION_SAMPLES = 512\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "QUANT_ENABLE = False\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"Preprocess dataset examples.\"\"\"\n",
    "    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\n",
    "\n",
    "\n",
    "def tokenize(sample):\n",
    "    \"\"\"Tokenize dataset examples.\"\"\"\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_recipe(fp8_enabled):\n",
    "    \"\"\"\n",
    "    Generate the compression recipe and save directory based on the FP8 flag.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_recipe = [\n",
    "        SparseGPTModifier(\n",
    "            sparsity=0.5,\n",
    "            mask_structure=\"2:4\",\n",
    "            targets=[r\"re:model.layers.\\d*$\"],\n",
    "        )\n",
    "    ]\n",
    "    save_dir = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"2of4-sparse\"\n",
    "\n",
    "    if fp8_enabled:\n",
    "        base_recipe.append(\n",
    "            QuantizationModifier(\n",
    "                targets=[\"Linear\"],\n",
    "                ignore=[\"lm_head\"],\n",
    "                scheme=\"FP8_DYNAMIC\",\n",
    "            )\n",
    "        )\n",
    "        save_dir = (\n",
    "            MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"2of4-W8A8-FP8-Dynamic-Per-Token\"\n",
    "        )\n",
    "\n",
    "        # check that asymmetric quantization is not being used\n",
    "        q_scheme = base_recipe[1].scheme\n",
    "        if not isinstance(q_scheme, str) and not q_scheme[\"weights\"].symmetric:\n",
    "            raise ValueError(\n",
    "                \"Asymmetric quantization with 2of4 sparsity is not supported by vLLM. \"\n",
    "                \"Please use symmetric quantization\"\n",
    "            )\n",
    "\n",
    "    return base_recipe, save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "ds = load_dataset(\n",
    "    DATASET_ID, \n",
    "    split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\"\n",
    ").shuffle(seed=47)\n",
    "ds = ds.map(preprocess)\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "\n",
    "# Get compression recipe and save directory\n",
    "recipe, save_dir = get_recipe(QUANT_ENABLE)\n",
    "\n",
    "# Apply compression\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "# Validate the compressed model\n",
    "print(\"\\n========== SAMPLE GENERATION ==============\")\n",
    "dispatch_for_generation(model)\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\n",
    "    model.device\n",
    ")\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# Save compressed model and tokenizer\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference pruned model via vLLM\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_path = r\"Llama-3.2-3B-Instruct2of4-sparse\"\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,    \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    enforce_eager=True,      # 호환성 모드 켜기 (필요시)\n",
    "    dtype='auto'\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=256\n",
    "    )\n",
    "\n",
    "prompt = \"Hello, My name is:\"\n",
    "\n",
    "outputs = model.generate(prompt, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
