{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12370997",
   "metadata": {},
   "source": [
    "# 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece6f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "Successfully installed pip-25.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting llmcompressor\n",
      "  Downloading llmcompressor-0.9.0.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: loguru<=0.7.3,>=0.7.2 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (0.7.3)\n",
      "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (6.0.2)\n",
      "Requirement already satisfied: numpy<=2.3.5,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.2.5)\n",
      "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.32.3)\n",
      "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (4.67.1)\n",
      "Requirement already satisfied: torch<=2.9.1,>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (2.9.1)\n",
      "Collecting transformers<=4.57.3,>=4.54.0 (from llmcompressor)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets<=4.4.1,>=4.0.0 (from llmcompressor)\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate<=1.12.0,>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (1.12.0)\n",
      "Collecting nvidia-ml-py<=13.590.44,>=12.560.30 (from llmcompressor)\n",
      "  Downloading nvidia_ml_py-13.590.44-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: pillow<=12.0.0,>=10.4.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (11.0.0)\n",
      "Requirement already satisfied: compressed-tensors==0.13.0 in /opt/conda/lib/python3.11/site-packages (from llmcompressor) (0.13.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.11/site-packages (from compressed-tensors==0.13.0->llmcompressor) (2.12.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (7.0.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (3.18.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (0.28.1)\n",
      "Collecting xxhash (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets<=4.4.1,>=4.0.0->llmcompressor)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate<=1.12.0,>=1.6.0->llmcompressor) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate<=1.12.0,>=1.6.0->llmcompressor) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/conda/lib/python3.11/site-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.57.3,>=4.54.0->llmcompressor) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.57.3,>=4.54.0->llmcompressor) (0.22.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.22.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch<=2.9.1,>=2.7.0->llmcompressor) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<=2.9.1,>=2.7.0->llmcompressor) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.17.0)\n",
      "Downloading llmcompressor-0.9.0.1-py3-none-any.whl (282 kB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading nvidia_ml_py-13.590.44-py3-none-any.whl (50 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: nvidia-ml-py, xxhash, pyarrow, dill, pandas, multiprocess, transformers, datasets, llmcompressor\n",
      "\u001b[2K  Attempting uninstall: nvidia-ml-py\n",
      "\u001b[2K    Found existing installation: nvidia-ml-py 13.590.48\n",
      "\u001b[2K    Uninstalling nvidia-ml-py-13.590.48:\n",
      "\u001b[2K      Successfully uninstalled nvidia-ml-py-13.590.48\u001b[32m0/9\u001b[0m [nvidia-ml-py]\n",
      "\u001b[2K  Attempting uninstall: dill0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/9\u001b[0m [pyarrow]\n",
      "\u001b[2K    Found existing installation: dill 0.4.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/9\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling dill-0.4.1:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.1━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [dill]\n",
      "\u001b[2K  Attempting uninstall: transformers90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/9\u001b[0m [multiprocess]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.6━━━━━━━━━━\u001b[0m \u001b[32m5/9\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling transformers-4.57.6:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.6━━━━━━━━━━━━\u001b[0m \u001b[32m6/9\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [llmcompressor]0m [llmcompressor]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.4.1 dill-0.4.0 llmcompressor-0.9.0.1 multiprocess-0.70.18 nvidia-ml-py-13.590.44 pandas-3.0.0 pyarrow-23.0.0 transformers-4.57.3 xxhash-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3461c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f90c1891d44000b716079e705c75c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "!pip install ipywidgets\n",
    "\n",
    "login()     # Your huggingface access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7f3c5",
   "metadata": {},
   "source": [
    "# 2. Compress Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a3356",
   "metadata": {},
   "source": [
    "## 2.1. Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fa194",
   "metadata": {},
   "source": [
    "See [Quantization Schemes](https://docs.vllm.ai/projects/llm-compressor/en/0.8.1/guides/compression_schemes/) for choosing quantization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a903a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT W8A8 quantization with SmoothQuant and GPTQ\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]\n",
    "\n",
    "oneshot(\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    dataset=\"open_platypus\",\n",
    "    recipe=recipe,\n",
    "    output_dir=\"TinyLlama-1.1B-Chat-v1.0-INT8\",\n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241dfbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 11:08:43 [utils.py:263] non-default args: {'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': './TinyLlama-1.1B-Chat-v1.0-INT8'}\n",
      "INFO 01-22 11:08:43 [model.py:530] Resolved architecture: LlamaForCausalLM\n",
      "INFO 01-22 11:08:43 [model.py:1545] Using max model len 2048\n",
      "INFO 01-22 11:08:44 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 01-22 11:08:44 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-22 11:08:44 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:44 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='./TinyLlama-1.1B-Chat-v1.0-INT8', speculative_config=None, tokenizer='./TinyLlama-1.1B-Chat-v1.0-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=./TinyLlama-1.1B-Chat-v1.0-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:44 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.3:55483 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:44 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:45 [gpu_model_runner.py:3808] Starting to load model ./TinyLlama-1.1B-Chat-v1.0-INT8...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:45 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:45 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d48b12f98046d695b30c9f68939b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:46 [default_loader.py:291] Loading weights took 0.21 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:46 [gpu_model_runner.py:3905] Model loading took 1.15 GiB memory and 0.460047 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:51 [backends.py:644] Using cache directory: /root/.cache/vllm/torch_compile_cache/c0332d605c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:51 [backends.py:704] Dynamo bytecode transform time: 4.47 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:55 [backends.py:261] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:59 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 6.62 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:08:59 [monitor.py:34] torch.compile takes 11.09 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:00 [gpu_worker.py:358] Available KV cache memory: 14.91 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:01 [kv_cache_utils.py:1305] GPU KV cache size: 710,736 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:01 [kv_cache_utils.py:1310] Maximum concurrency for 2,048 tokens per request: 347.04x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:00<00:00, 61.46it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 60.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:03 [gpu_model_runner.py:4856] Graph capturing finished in 2 secs, took 0.44 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=925)\u001b[0;0m INFO 01-22 11:09:03 [core.py:273] init engine (profile, create kv cache, warmup model) took 16.58 seconds\n",
      "INFO 01-22 11:09:03 [llm.py:347] Supported tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6783f35a4484be9b5d06d14d0b0ded0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad2dbf517934a88b8c9a88ed52cdaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.2 How does machine learning work in image classification?\n",
      "2.3 How does machine learning perform better than human classification algorithms?\n",
      "2.4 How is model selection and evaluation performed in automated machine learning systems?\n",
      "2.5 What are the pros and cons of using machine learning algorithms for image classification?\n",
      "3.1 How is data preprocessing used in machine learning algorithms?\n",
      "3.2 What is the DL4J framework for data preprocessing in machine learning?\n",
      "3.3 How does the KNN algorithm perform in data preprocessing and classification tasks?\n",
      "3.4 How does the Naive Bayes algorithm perform in data preprocessing and classification tasks?\n",
      "3.5 What sets Noah's Alternative framework apart from other machine learning frameworks for image classification?\n",
      "\n",
      "Learning outcomes:\n",
      "1. Understanding the definition of machine learning, its various types of algorithms, and their applications.\n",
      "2. Understanding how machine learning works in image classification.\n",
      "3. Identifying the role of preprocessing in machine learning algorithms.\n",
      "4. Understanding the DasTDL framework for data preprocessing in machine learning.\n",
      "5. Analyzing the pros and cons of using machine learning algorithms for image classification\n"
     ]
    }
   ],
   "source": [
    "# Inference quantized model via vLLM\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_path = \"./TinyLlama-1.1B-Chat-v1.0-INT8\"\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,    \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    # enforce_eager=True      # 호환성 모드 켜기 (필요시)\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=256)\n",
    "\n",
    "outputs = model.generate(\"What is machine learning?\", sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92d6f0",
   "metadata": {},
   "source": [
    "## 2.2. Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9767f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:4 sparse pruning with/without FP8 quantization\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.obcq import SparseGPTModifier\n",
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor.utils import dispatch_for_generation\n",
    "\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "DATASET_ID = \"HuggingFaceH4/ultrachat_200k\"\n",
    "DATASET_SPLIT = \"train_sft\"\n",
    "NUM_CALIBRATION_SAMPLES = 512\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "QUANT_ENABLE = False\n",
    "\n",
    "def preprocess(example):\n",
    "    \"\"\"Preprocess dataset examples.\"\"\"\n",
    "    return {\"text\": tokenizer.apply_chat_template(example[\"messages\"], tokenize=False)}\n",
    "\n",
    "\n",
    "def tokenize(sample):\n",
    "    \"\"\"Tokenize dataset examples.\"\"\"\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_recipe(fp8_enabled):\n",
    "    \"\"\"\n",
    "    Generate the compression recipe and save directory based on the FP8 flag.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_recipe = [\n",
    "        SparseGPTModifier(\n",
    "            sparsity=0.5,\n",
    "            mask_structure=\"2:4\",\n",
    "            targets=[r\"re:model.layers.\\d*$\"],\n",
    "        )\n",
    "    ]\n",
    "    save_dir = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"2of4-sparse\"\n",
    "\n",
    "    if fp8_enabled:\n",
    "        base_recipe.append(\n",
    "            QuantizationModifier(\n",
    "                targets=[\"Linear\"],\n",
    "                ignore=[\"lm_head\"],\n",
    "                scheme=\"FP8_DYNAMIC\",\n",
    "            )\n",
    "        )\n",
    "        save_dir = (\n",
    "            MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"2of4-W8A8-FP8-Dynamic-Per-Token\"\n",
    "        )\n",
    "\n",
    "        # check that asymmetric quantization is not being used\n",
    "        q_scheme = base_recipe[1].scheme\n",
    "        if not isinstance(q_scheme, str) and not q_scheme[\"weights\"].symmetric:\n",
    "            raise ValueError(\n",
    "                \"Asymmetric quantization with 2of4 sparsity is not supported by vLLM. \"\n",
    "                \"Please use symmetric quantization\"\n",
    "            )\n",
    "\n",
    "    return base_recipe, save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "ds = load_dataset(\n",
    "    DATASET_ID, \n",
    "    split=f\"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\"\n",
    ").shuffle(seed=47)\n",
    "ds = ds.map(preprocess)\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "\n",
    "# Get compression recipe and save directory\n",
    "recipe, save_dir = get_recipe(QUANT_ENABLE)\n",
    "\n",
    "# Apply compression\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "# Validate the compressed model\n",
    "print(\"\\n========== SAMPLE GENERATION ==============\")\n",
    "dispatch_for_generation(model)\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\n",
    "    model.device\n",
    ")\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# Save compressed model and tokenizer\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dd970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference pruned model via vLLM\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_path = r\"Llama-3.2-3B-Instruct2of4-sparse\"\n",
    "\n",
    "model = LLM(\n",
    "    model=model_path,    \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    enforce_eager=True,      # 호환성 모드 켜기 (필요시)\n",
    "    dtype='auto'\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=256\n",
    "    )\n",
    "\n",
    "prompt = \"Hello, My name is:\"\n",
    "\n",
    "outputs = model.generate(prompt, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
