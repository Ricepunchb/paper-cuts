{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac3f0ef",
   "metadata": {},
   "source": [
    "# 1. Setting Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b901ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vllm\n",
      "  Downloading vllm-0.14.0-cp38-abi3-manylinux_2_31_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.11/site-packages (from vllm) (2026.1.15)\n",
      "Collecting cachetools (from vllm)\n",
      "  Downloading cachetools-6.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from vllm) (7.0.0)\n",
      "Collecting sentencepiece (from vllm)\n",
      "  Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from vllm) (2.2.5)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from vllm) (4.67.1)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading blake3-1.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting py-cpuinfo (from vllm)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: transformers<5,>=4.56.0 in /opt/conda/lib/python3.11/site-packages (from vllm) (4.57.6)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /opt/conda/lib/python3.11/site-packages (from vllm) (0.22.2)\n",
      "Collecting protobuf>=6.30.0 (from vllm)\n",
      "  Downloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi-0.128.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting aiohttp (from vllm)\n",
      "  Downloading aiohttp-3.13.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting openai>=1.99.1 (from vllm)\n",
      "  Downloading openai-2.15.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pydantic>=2.12.0 (from vllm)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting prometheus_client>=0.18.0 (from vllm)\n",
      "  Downloading prometheus_client-0.24.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from vllm) (11.0.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer==0.11.3 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting llguidance<1.4.0,>=1.3.0 (from vllm)\n",
      "  Downloading llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting outlines_core==0.2.11 (from vllm)\n",
      "  Downloading outlines_core-0.2.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting diskcache==5.6.3 (from vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lark==1.2.2 (from vllm)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.29 (from vllm)\n",
      "  Downloading xgrammar-0.1.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /opt/conda/lib/python3.11/site-packages (from vllm) (4.13.2)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /opt/conda/lib/python3.11/site-packages (from vllm) (3.18.0)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /opt/conda/lib/python3.11/site-packages (from vllm) (27.1.0)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading msgspec-0.20.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting gguf>=0.17.0 (from vllm)\n",
      "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting mistral_common>=1.8.8 (from mistral_common[image]>=1.8.8->vllm)\n",
      "  Downloading mistral_common-1.8.8-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.11.0 (from vllm)\n",
      "  Downloading opencv_python_headless-4.13.0.90-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from vllm) (6.0.2)\n",
      "Collecting einops (from vllm)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting compressed-tensors==0.13.0 (from vllm)\n",
      "  Downloading compressed_tensors-0.13.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting depyf==0.20.0 (from vllm)\n",
      "  Downloading depyf-0.20.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading watchfiles-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting python-json-logger (from vllm)\n",
      "  Downloading python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from vllm) (1.11.1.4)\n",
      "Collecting pybase64 (from vllm)\n",
      "  Downloading pybase64-1.4.3-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting cbor2 (from vllm)\n",
      "  Downloading cbor2-5.8.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting ijson (from vllm)\n",
      "  Downloading ijson-3.4.0.post0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "Collecting setproctitle (from vllm)\n",
      "  Downloading setproctitle-1.3.7-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
      "Collecting openai-harmony>=0.0.3 (from vllm)\n",
      "  Downloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting anthropic==0.71.0 (from vllm)\n",
      "  Downloading anthropic-0.71.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting model-hosting-container-standards<1.0.0,>=0.1.10 (from vllm)\n",
      "  Downloading model_hosting_container_standards-0.1.13-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting mcp (from vllm)\n",
      "  Downloading mcp-1.25.0-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting grpcio>=1.76.0 (from vllm)\n",
      "  Downloading grpcio-1.76.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting grpcio-reflection>=1.76.0 (from vllm)\n",
      "  Downloading grpcio_reflection-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting numba==0.61.2 (from vllm)\n",
      "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting ray>=2.48.0 (from ray[cgraph]>=2.48.0->vllm)\n",
      "  Downloading ray-2.53.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting torch==2.9.1 (from vllm)\n",
      "  Downloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchaudio==2.9.1 (from vllm)\n",
      "  Downloading torchaudio-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting torchvision==0.24.1 (from vllm)\n",
      "  Downloading torchvision-0.24.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting flashinfer-python==0.5.3 (from vllm)\n",
      "  Downloading flashinfer_python-0.5.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from anthropic==0.71.0->vllm) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from anthropic==0.71.0->vllm) (1.9.0)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic==0.71.0->vllm)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from anthropic==0.71.0->vllm) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic==0.71.0->vllm)\n",
      "  Downloading jiter-0.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from anthropic==0.71.0->vllm)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting loguru (from compressed-tensors==0.13.0->vllm)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting astor (from depyf==0.20.0->vllm)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dill (from depyf==0.20.0->vllm)\n",
      "  Downloading dill-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting apache-tvm-ffi<0.2,>=0.1 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading apache_tvm_ffi-0.1.8.post2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from flashinfer-python==0.5.3->vllm) (8.1.8)\n",
      "Collecting nvidia-cudnn-frontend>=1.13.0 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading nvidia_cudnn_frontend-1.17.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting nvidia-cutlass-dsl>=4.2.1 (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading nvidia_cutlass_dsl-4.3.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-ml-py (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading nvidia_ml_py-13.590.48-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: packaging>=24.2 in /opt/conda/lib/python3.11/site-packages (from flashinfer-python==0.5.3->vllm) (24.2)\n",
      "Collecting tabulate (from flashinfer-python==0.5.3->vllm)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.11.3->vllm)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.9.1->vllm) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.9.1->vllm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.9.1->vllm) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.11/site-packages (from torch==2.9.1->vllm) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.1->vllm)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch==2.9.1->vllm)\n",
      "  Downloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting starlette<0.51.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastapi-cli>=0.0.8 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cli-0.0.20-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading python_multipart-0.0.21-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pydantic-settings>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pydantic-extra-types>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading pydantic_extra_types-2.11.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /opt/conda/lib/python3.11/site-packages (from mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (4.23.0)\n",
      "Collecting jmespath (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm) (75.8.2)\n",
      "Collecting supervisor>=4.2.0 (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm)\n",
      "  Downloading supervisor-4.3.0-py2.py3-none-any.whl.metadata (87 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.12.0->vllm)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.12.0->vllm)\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing_extensions>=4.10 (from vllm)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.12.0->vllm)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from ray>=2.48.0->ray[cgraph]>=2.48.0->vllm) (1.1.0)\n",
      "Requirement already satisfied: cupy-cuda12x in /opt/conda/lib/python3.11/site-packages (from ray[cgraph]>=2.48.0->vllm) (13.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->vllm) (2026.1.4)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/conda/lib/python3.11/site-packages (from tokenizers>=0.21.1->vllm) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers<5,>=4.56.0->vllm) (0.7.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->vllm)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->vllm)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->vllm) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->vllm)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->vllm)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->vllm)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->vllm)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting httpx-sse>=0.4 (from mcp->vllm)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting pyjwt>=2.10.1 (from pyjwt[crypto]>=2.10.1->mcp->vllm)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp->vllm)\n",
      "  Downloading sse_starlette-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
      "Collecting typer>=0.15.1 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading typer-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting rich-toolkit>=0.14.8 (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich_toolkit-0.17.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting fastapi-cloud-cli>=0.1.1 (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastapi_cloud_cli-0.11.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic==0.71.0->vllm) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.21.1->vllm) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.9.1->vllm) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (0.24.0)\n",
      "Collecting cuda-python>=12.8 (from nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm)\n",
      "  Downloading cuda_python-13.1.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting cryptography>=3.4.0 (from pyjwt[crypto]>=2.10.1->mcp->vllm)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.9.1->vllm) (1.3.0)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading httptools-0.7.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading uvloop-0.22.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading websockets-16.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /opt/conda/lib/python3.11/site-packages (from cupy-cuda12x->ray[cgraph]>=2.48.0->vllm) (0.8.3)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm)\n",
      "  Downloading cffi-2.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting cuda-bindings~=13.1.1 (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm)\n",
      "  Downloading cuda_bindings-13.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-python>=12.8->nvidia-cutlass-dsl>=4.2.1->flashinfer-python==0.5.3->vllm)\n",
      "  Downloading cuda_pathfinder-1.3.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting rignore>=0.5.1 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rignore-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting sentry-sdk>=2.20.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading sentry_sdk-2.50.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fastar>=0.8.0 (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading fastar-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting rich>=13.7.1 (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading vllm-0.14.0-cp38-abi3-manylinux_2_31_x86_64.whl (495.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.4/495.4 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anthropic-0.71.0-py3-none-any.whl (355 kB)\n",
      "Downloading compressed_tensors-0.13.0-py3-none-any.whl (192 kB)\n",
      "Downloading depyf-0.20.0-py3-none-any.whl (39 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading flashinfer_python-0.5.3-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n",
      "Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading outlines_core-0.2.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.24.1-cp311-cp311-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgrammar-0.1.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.128.0-py3-none-any.whl (103 kB)\n",
      "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
      "Downloading grpcio-1.76.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_reflection-1.76.0-py3-none-any.whl (22 kB)\n",
      "Downloading llguidance-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistral_common-1.8.8-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading model_hosting_container_standards-0.1.13-py3-none-any.whl (105 kB)\n",
      "Downloading openai-2.15.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai_harmony-0.0.8-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.13.0.90-cp37-abi3-manylinux_2_28_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_client-0.24.1-py3-none-any.whl (64 kB)\n",
      "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ray-2.53.0-cp311-cp311-manylinux2014_x86_64.whl (72.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.12.0-cp311-cp311-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading aiohttp-3.13.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blake3-1.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (387 kB)\n",
      "Downloading cachetools-6.2.4-py3-none-any.whl (11 kB)\n",
      "Downloading cbor2-5.8.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (258 kB)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading ijson-3.4.0.post0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (134 kB)\n",
      "Downloading mcp-1.25.0-py3-none-any.whl (233 kB)\n",
      "Downloading msgspec-0.20.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (219 kB)\n",
      "Downloading partial_json_parser-0.2.1.1.post7-py3-none-any.whl (10 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pybase64-1.4.3-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
      "Downloading python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
      "Downloading sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.7-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
      "Downloading watchfiles-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading apache_tvm_ffi-0.1.8.post2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
      "Downloading fastapi_cli-0.0.20-py3-none-any.whl (12 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jiter-0.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading nvidia_cudnn_frontend-1.17.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cutlass_dsl-4.3.5-cp311-cp311-manylinux_2_28_x86_64.whl (58.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading pydantic_extra_types-2.11.0-py3-none-any.whl (74 kB)\n",
      "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading python_multipart-0.0.21-py3-none-any.whl (24 kB)\n",
      "Downloading sse_starlette-3.2.0-py3-none-any.whl (12 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Downloading supervisor-4.3.0-py2.py3-none-any.whl (320 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.40.0-py3-none-any.whl (68 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading dill-0.4.1-py3-none-any.whl (120 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading nvidia_ml_py-13.590.48-py3-none-any.whl (50 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_python-13.1.1-py3-none-any.whl (8.0 kB)\n",
      "Downloading fastapi_cloud_cli-0.11.0-py3-none-any.whl (26 kB)\n",
      "Downloading httptools-0.7.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (456 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading rich_toolkit-0.17.1-py3-none-any.whl (31 kB)\n",
      "Downloading typer-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading uvloop-0.22.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-16.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (184 kB)\n",
      "Downloading cffi-2.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (215 kB)\n",
      "Downloading cuda_bindings-13.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)\n",
      "Downloading fastar-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (821 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading rignore-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (959 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.5/959.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.50.0-py2.py3-none-any.whl (424 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: supervisor, py-cpuinfo, nvidia-ml-py, nvidia-cusparselt-cu12, websockets, uvloop, uvicorn, typing_extensions, triton, tabulate, sniffio, shellingham, setproctitle, sentry-sdk, sentencepiece, rignore, python-multipart, python-json-logger, python-dotenv, pyjwt, pycountry, pybase64, protobuf, propcache, prometheus_client, partial-json-parser, outlines_core, opencv-python-headless, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cudnn-frontend, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multidict, msgspec, mdurl, loguru, llvmlite, llguidance, lark, jmespath, jiter, interegular, ijson, httpx-sse, httptools, gguf, frozenlist, fastar, email-validator, einops, docstring-parser, diskcache, dill, cuda-pathfinder, cloudpickle, cffi, cbor2, cachetools, astor, annotated-types, annotated-doc, aiohappyeyeballs, yarl, typing-inspection, tiktoken, pydantic-core, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, markdown-it-py, grpcio, depyf, cuda-bindings, cryptography, blake3, apache-tvm-ffi, aiosignal, watchfiles, starlette, rich, pydantic, nvidia-cusolver-cu12, grpcio-reflection, cuda-python, aiohttp, typer, torch, sse-starlette, rich-toolkit, pydantic-settings, pydantic-extra-types, prometheus-fastapi-instrumentator, openai-harmony, openai, nvidia-cutlass-dsl, lm-format-enforcer, fastapi, anthropic, xgrammar, torchvision, torchaudio, ray, model-hosting-container-standards, mcp, flashinfer-python, fastapi-cloud-cli, fastapi-cli, compressed-tensors, mistral_common, vllm\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.0\n",
      "    Uninstalling triton-3.3.0:\n",
      "      Successfully uninstalled triton-3.3.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.8.55\n",
      "    Uninstalling nvidia-nvtx-cu12-12.8.55:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.8.55\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.61\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.61:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.61\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
      "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.55\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.55:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.55\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.13.0.11\n",
      "    Uninstalling nvidia-cufile-cu12-1.13.0.11:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.13.0.11\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.57\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.57:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.57\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.61\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.61:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.61\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.57\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.8.57:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.57\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.3.14\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.3.14:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.3.14\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.17.1\n",
      "    Uninstalling cffi-1.17.1:\n",
      "      Successfully uninstalled cffi-1.17.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.7.53\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.7.53:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.7.53\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.41\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.41:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.41\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.7.1.26\n",
      "    Uninstalling nvidia-cudnn-cu12-9.7.1.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.7.1.26\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.2.55\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.2.55:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.2.55\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.0+cu128\n",
      "    Uninstalling torch-2.7.0+cu128:\n",
      "      Successfully uninstalled torch-2.7.0+cu128\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.22.0+cu128\n",
      "    Uninstalling torchvision-0.22.0+cu128:\n",
      "      Successfully uninstalled torchvision-0.22.0+cu128\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.7.0+cu128\n",
      "    Uninstalling torchaudio-2.7.0+cu128:\n",
      "      Successfully uninstalled torchaudio-2.7.0+cu128\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 annotated-doc-0.0.4 annotated-types-0.7.0 anthropic-0.71.0 apache-tvm-ffi-0.1.8.post2 astor-0.8.1 blake3-1.0.8 cachetools-6.2.4 cbor2-5.8.0 cffi-2.0.0 cloudpickle-3.1.2 compressed-tensors-0.13.0 cryptography-46.0.3 cuda-bindings-13.1.1 cuda-pathfinder-1.3.3 cuda-python-13.1.1 depyf-0.20.0 dill-0.4.1 diskcache-5.6.3 docstring-parser-0.17.0 einops-0.8.1 email-validator-2.3.0 fastapi-0.128.0 fastapi-cli-0.0.20 fastapi-cloud-cli-0.11.0 fastar-0.8.0 flashinfer-python-0.5.3 frozenlist-1.8.0 gguf-0.17.1 grpcio-1.76.0 grpcio-reflection-1.76.0 httptools-0.7.1 httpx-sse-0.4.3 ijson-3.4.0.post0 interegular-0.3.3 jiter-0.12.0 jmespath-1.0.1 lark-1.2.2 llguidance-1.3.0 llvmlite-0.44.0 lm-format-enforcer-0.11.3 loguru-0.7.3 markdown-it-py-4.0.0 mcp-1.25.0 mdurl-0.1.2 mistral_common-1.8.8 model-hosting-container-standards-0.1.13 msgspec-0.20.0 multidict-6.7.0 numba-0.61.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cudnn-frontend-1.17.0 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-cutlass-dsl-4.3.5 nvidia-ml-py-13.590.48 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 openai-2.15.0 openai-harmony-0.0.8 opencv-python-headless-4.13.0.90 outlines_core-0.2.11 partial-json-parser-0.2.1.1.post7 prometheus-fastapi-instrumentator-7.1.0 prometheus_client-0.24.1 propcache-0.4.1 protobuf-6.33.4 py-cpuinfo-9.0.0 pybase64-1.4.3 pycountry-24.6.1 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-extra-types-2.11.0 pydantic-settings-2.12.0 pyjwt-2.10.1 python-dotenv-1.2.1 python-json-logger-4.0.0 python-multipart-0.0.21 ray-2.53.0 rich-14.2.0 rich-toolkit-0.17.1 rignore-0.7.6 sentencepiece-0.2.1 sentry-sdk-2.50.0 setproctitle-1.3.7 shellingham-1.5.4 sniffio-1.3.1 sse-starlette-3.2.0 starlette-0.50.0 supervisor-4.3.0 tabulate-0.9.0 tiktoken-0.12.0 torch-2.9.1 torchaudio-2.9.1 torchvision-0.24.1 triton-3.5.1 typer-0.21.1 typing-inspection-0.4.2 typing_extensions-4.15.0 uvicorn-0.40.0 uvloop-0.22.1 vllm-0.14.0 watchfiles-1.1.1 websockets-16.0 xgrammar-0.1.29 yarl-1.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 07:54:46 [utils.py:263] non-default args: {'disable_log_stats': True, 'model': 'facebook/opt-125m'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9acb4e7b254b339e4fd4f5243030c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 07:54:53 [model.py:530] Resolved architecture: OPTForCausalLM\n",
      "INFO 01-22 07:54:53 [model.py:1545] Using max model len 2048\n",
      "INFO 01-22 07:54:54 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 01-22 07:54:54 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-22 07:54:54 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e1484ea7ee40fbbc2fe7a4a75c550f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550a60a850fa4ec584b8ab02929e218a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53e264ff5564d01bd2b5281abc095ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed6d5e14aee43eeb6e94f5866d44ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9956ebbadb845cd96d13d7963a80a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:54:59 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=facebook/opt-125m, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:54:59 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.3:42547 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:00 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:00 [gpu_model_runner.py:3808] Starting to load model facebook/opt-125m...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:14 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1002e51e0da4f7d9f66b867ec456dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:33 [weight_utils.py:510] Time spent downloading weights for facebook/opt-125m: 18.167030 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee5101a84c9445d9cb2a0432d3665d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:33 [default_loader.py:291] Loading weights took 0.19 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:33 [gpu_model_runner.py:3905] Model loading took 0.24 GiB memory and 32.655091 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:35 [backends.py:644] Using cache directory: /root/.cache/vllm/torch_compile_cache/3447fac536/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:35 [backends.py:704] Dynamo bytecode transform time: 1.22 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:36 [backends.py:261] Cache the graph of compile range (1, 8192) for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:38 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 2.73 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:38 [monitor.py:34] torch.compile takes 3.95 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:39 [gpu_worker.py:358] Available KV cache memory: 20.46 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:39 [kv_cache_utils.py:1305] GPU KV cache size: 595,888 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:39 [kv_cache_utils.py:1310] Maximum concurrency for 2,048 tokens per request: 290.96x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:00<00:00, 70.55it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 91.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:40 [gpu_model_runner.py:4856] Graph capturing finished in 2 secs, took 0.19 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=77882)\u001b[0;0m INFO 01-22 07:55:40 [core.py:273] init engine (profile, create kv cache, warmup model) took 7.15 seconds\n",
      "INFO 01-22 07:55:41 [llm.py:347] Supported tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f0e27d6c624c51b7a112622dc7787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4ee6704e184c858caf9ff0a1547bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is', Generated text: ' Joel, my dad is my friend and we are in a relationship. I am'\n",
      "Prompt: 'The president of the United States is', Generated text: ' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws in'\n",
      "Prompt: 'The capital of France is', Generated text: ' all set to open a new gateway to the most notorious terror camps in Europe,'\n",
      "Prompt: 'The future of AI is', Generated text: ' pretty much in the hands of the consumer. Everything in the world has a competitive'\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "llm = LLM(model=\"facebook/opt-125m\")    # Browse supported models https://docs.vllm.ai/en/latest/models/supported_models/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecbe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16163dc6",
   "metadata": {},
   "source": [
    "## Any other model from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd9337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LGAI-EXAONE/EXAONE-4.0-1.2B with vLLM...\n",
      "INFO 01-22 08:21:55 [utils.py:263] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'LGAI-EXAONE/EXAONE-4.0-1.2B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 08:21:57 [model.py:530] Resolved architecture: Exaone4ForCausalLM\n",
      "INFO 01-22 08:21:57 [model.py:1545] Using max model len 65536\n",
      "INFO 01-22 08:21:57 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 01-22 08:21:57 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-22 08:21:57 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n",
      "WARNING 01-22 08:21:57 [vllm.py:665] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 01-22 08:21:57 [vllm.py:765] Cudagraph is disabled under eager mode\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:21:59 [core.py:97] Initializing a V1 LLM engine (v0.14.0) with config: model='LGAI-EXAONE/EXAONE-4.0-1.2B', speculative_config=None, tokenizer='LGAI-EXAONE/EXAONE-4.0-1.2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=LGAI-EXAONE/EXAONE-4.0-1.2B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:00 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.3:60833 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:00 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:01 [gpu_model_runner.py:3808] Starting to load model LGAI-EXAONE/EXAONE-4.0-1.2B...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:01 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:02 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1b5728e149409ca300a28c26a1b475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:02 [default_loader.py:291] Loading weights took 0.40 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:03 [gpu_model_runner.py:3905] Model loading took 2.39 GiB memory and 1.515960 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:04 [gpu_worker.py:358] Available KV cache memory: 13.1 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:04 [kv_cache_utils.py:1305] GPU KV cache size: 228,976 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:04 [kv_cache_utils.py:1310] Maximum concurrency for 65,536 tokens per request: 3.49x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:04 [core.py:273] init engine (profile, create kv cache, warmup model) took 1.25 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m WARNING 01-22 08:22:04 [vllm.py:672] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=95059)\u001b[0;0m INFO 01-22 08:22:04 [vllm.py:765] Cudagraph is disabled under eager mode\n",
      "INFO 01-22 08:22:05 [llm.py:347] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "model_id = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"        # Model ID from huggingface\n",
    "\n",
    "print(f\"Loading {model_id} with vLLM...\")\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_id,\n",
    "    trust_remote_code=True,     # 커스텀 코드 신뢰 필요시\n",
    "    dtype=\"bfloat16\",        \n",
    "    gpu_memory_utilization=0.7, # GPU 메모리 70% 사용 (필요시 조절)\n",
    "    tensor_parallel_size=1,   # GPU 1개 사용\n",
    "    enforce_eager=True      # 호환성 모드 켜기 (필요시)\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97302c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Formatted Prompt Example:\n",
      "[|user|]\n",
      "너가 얼마나 대단한지 설명해 봐[|endofturn|]\n",
      "[|assistant|]\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd8b52d2949462a996fe644209ee718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73bc8c3a87845c8b5fb5c1b8e27ce73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q]: [|user|]\n",
      "너가 얼마나 대단한지 설명해 봐[|endofturn|]\n",
      "[|assistant|]\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "[A]: 저는 EXAONE으로, LG AI Research에서 개발된 Large Language Model입니다. 제 성능은 다음과 같은 기준에 따라 평가됩니다:\n",
      "\n",
      "1. **학습 데이터**: 방대한 양의 한국어 텍스트 데이터를 통해 학습되었습니다. 이는 다양한 언어와 주제에 대한 포괄적인 이해를 제공합니다.\n",
      "\n",
      "2. **이해도와 생성 능력**: 복잡한 문장 구조와 문맥을 이해하며, 요약, 번역, 창의적인 글쓰기 등 다양한 작업을 정확하고 효과적으로 수행합니다.\n",
      "\n",
      "3. **실용성**: 실제 문제 해결에 적용 가능한 결과를 제공하며, 사용자의 요구에 민감하게 대응합니다.\n",
      "\n",
      "4. **개선 가능성**: 지속적인 업데이트와 학습을 통해 발전하고 있어, 기술 분야에서 지속적인 혁신의 주체가 되고 있습니다.\n",
      "\n",
      "이러한 요소들을 종합하면, 저는 높은 수준의 인공지능 도구로서 가치 있는 존재라고 생각합니다.\n",
      "--------------------------------------------------\n",
      "\n",
      "[Q]: [|user|]\n",
      "Python으로 피보나치 수열 짜는 코드 보여줘.[|endofturn|]\n",
      "[|assistant|]\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "[A]: # Python으로 피보나치 수열 생성 코드\n",
      "\n",
      "## 반복문을 사용한 기본 구현\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    \"\"\"0부터 n까지의 피보나치 수열 생성\"\"\"\n",
      "    sequence = []\n",
      "    a, b = 0, 1\n",
      "    for _ in range(n + 1):\n",
      "        sequence.append(a)\n",
      "        a, b = b, a + b\n",
      "    return sequence\n",
      "\n",
      "# 10까지 계산해보기\n",
      "print(fibonacci(10))\n",
      "```\n",
      "\n",
      "## 재귀를 사용한 구현\n",
      "```python\n",
      "def recursive_fibonacci(n):\n",
      "    \"\"\"재귀적으로 피보나치 수열 생성\"\"\"\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return recursive_fibonacci(n-1) + recursive_fibonacci(n-2)\n",
      "\n",
      "# 결과 출력\n",
      "print(recursive_fibonacci(10))\n",
      "```\n",
      "\n",
      "## 리스트 컴프리헨션을 사용한 구현\n",
      "```python\n",
      "def fibonacci_comp(n):\n",
      "    \"\"\"리스트 컴프리헨션으로 간결하게 구현\"\"\"\n",
      "    fib_sequence = [0, \n",
      "--------------------------------------------------\n",
      "\n",
      "[Q]: [|user|]\n",
      "서울의 맛집을 추천해줘.[|endofturn|]\n",
      "[|assistant|]\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "[A]: 서울은 다양한 맛과 분위기를 즐길 수 있는 도시입니다. 다음은 몇 가지 추천 맛집을 소개해 드릴게요:\n",
      "\n",
      "### 1. **서울식 떡볶이집**\n",
      "   - **키친투데이** (서울특별시 내면재털 spaces)  \n",
      "     고소한 재료와 다양한 떡볶이 종류를 경험할 수 있는 유명한 곳. 특히 \"라면 떡볶이\"가 인기 만점이에요.\n",
      "   - **스리파정 떡볶이** (서울특별시 친화정)  \n",
      "     전통적인 서울 맛을 현대적으로 재해석한 떡볶이로, 가족 모두가 좋아하는 곳입니다.\n",
      "\n",
      "### 2. **고급형 한식족 맛집**\n",
      "   - **해녀정식** (서울특별시 팔구빌딩)  \n",
      "     해녀 요리와 한국 전통 음식이 결합된 독특한 메뉴로 유명합니다.\n",
      "   - **보람의집** (서울특별시 종로구)  \n",
      "     국내 최고의 한식 레스토랑 중 하나로, 테이스팅 메뉴도 좋습니다.\n",
      "\n",
      "### 3. **Ватк한 카페 & 디저트 spot**\n",
      "   - **커피앤창작** (서울특별시 synagogue 앞)  \n",
      "     건강한 원두와 디저트가 분위기 있는 카페\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "conversations = [\n",
    "    [{\"role\": \"user\", \"content\": \"너가 얼마나 대단한지 설명해 봐\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Python으로 피보나치 수열 짜는 코드 보여줘.\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"서울의 맛집을 추천해줘.\"}]\n",
    "]\n",
    "\n",
    "formatted_prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        conversation, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True # 답변을 시작할 준비를 시킴 (Assistant 태그 추가)\n",
    "    ) \n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "# (디버깅용) 실제로 모델에 들어가는 프롬프트가 어떻게 생겼는지 하나만 출력해봄\n",
    "print(\">>> Formatted Prompt Example:\")\n",
    "print(formatted_prompts[0])\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 샘플링 파라미터 (창의성 조절)\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=256,\n",
    "    stop=[\"<|endoftext|>\"]      # EXAONE의 끝 토큰\n",
    ")\n",
    "\n",
    "outputs = llm.generate(formatted_prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"\\n[Q]: {prompt}\\n[A]: {generated_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
