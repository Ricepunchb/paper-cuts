{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463e03cb",
   "metadata": {},
   "source": [
    "# 1. How to use pre-compressed(dFloat11 type) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e148d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dfloat11[cuda12]\n",
      "  Downloading dfloat11-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting accelerate (from dfloat11[cuda12])\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting dahuffman==0.4.2 (from dfloat11[cuda12])\n",
      "  Downloading dahuffman-0.4.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting huggingface-hub (from dfloat11[cuda12])\n",
      "  Downloading huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from dfloat11[cuda12])\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting transformers (from dfloat11[cuda12])\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from dfloat11[cuda12]) (4.67.1)\n",
      "Collecting cupy-cuda12x (from dfloat11[cuda12])\n",
      "  Downloading cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate->dfloat11[cuda12]) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate->dfloat11[cuda12]) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->dfloat11[cuda12]) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate->dfloat11[cuda12]) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate->dfloat11[cuda12]) (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->dfloat11[cuda12]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->dfloat11[cuda12]) (2025.3.2)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub->dfloat11[cuda12])\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub->dfloat11[cuda12])\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub->dfloat11[cuda12])\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub->dfloat11[cuda12])\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->dfloat11[cuda12]) (4.13.2)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x->dfloat11[cuda12])\n",
      "  Downloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub (from dfloat11[cuda12])\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers->dfloat11[cuda12])\n",
      "  Downloading regex-2026.1.15-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers->dfloat11[cuda12]) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers->dfloat11[cuda12])\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate->dfloat11[cuda12]) (75.8.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->dfloat11[cuda12]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->dfloat11[cuda12]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->dfloat11[cuda12]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers->dfloat11[cuda12]) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate->dfloat11[cuda12]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate->dfloat11[cuda12]) (3.0.2)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub->dfloat11[cuda12])\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub->dfloat11[cuda12])\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub->dfloat11[cuda12])\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer-slim->huggingface-hub->dfloat11[cuda12]) (8.1.8)\n",
      "Downloading dahuffman-0.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl (113.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dfloat11-0.5.0-py3-none-any.whl (24 kB)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (54 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fastrlock, safetensors, regex, hf-xet, dahuffman, cupy-cuda12x, huggingface-hub, tokenizers, transformers, accelerate, dfloat11\n",
      "Successfully installed accelerate-1.12.0 cupy-cuda12x-13.6.0 dahuffman-0.4.2 dfloat11-0.5.0 fastrlock-0.8.3 hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2026.1.15 safetensors-0.7.0 tokenizers-0.22.2 transformers-4.57.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install dfloat11 from pypi\n",
    "!pip install -U dfloat11[cuda12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b682e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 47 files: 100%|██████████| 47/47 [05:30<00:00,  7.03s/it]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading DFloat11 safetensors: 100%|██████████| 37/37 [00:00<00:00, 60.48it/s]\n",
      "Total model size: 6.5031 GB\n",
      "Allocated 100925440 bf16 on device cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Question: What is a binary tree and its applications? Answer: A binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child. Binary trees are used in various applications such as binary search trees (BSTs) for efficient searching, sorting, and insertion operations. They are also used in data compression algorithms like Huffman coding, and in expression trees for representing arithmetic expressions. Additionally, binary trees are used in graph algorithms for traversal and searching.\\n\\nAnswer: A binary tree is a tree data structure where each node has at most two children, referred to as the left and right child. They are used in various applications such as binary search trees (BSTs) for efficient searching, sorting, and insertion. Binary trees are also used in data compression algorithms like Huffman coding, and in expression trees for representing arithmetic expressions. Additionally, binary trees are used in graph algorithms for traversal and searching.\\n\\nTo create a binary tree in Python, you can use a class with left and right child attributes. Here's an example:\\n\\n```python\\nclass Node:\\n    def __init__(self, value):\\n        self.value = value\\n        self.left = None\\n        self.right = None\\n\\n# Creating a binary tree\\nroot = Node(1)\\nroot.left = Node\"]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dfloat11 import DFloat11Model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"DFloat11/Qwen3-4B-DF11\" # HuggingFace name or local path of the DFloat11 model. Browse from https://github.com/LeanModels/DFloat11?tab=readme-ov-file#-model-hub\n",
    "\n",
    "model = DFloat11Model.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt = \"Question: What is a binary tree and its applications? Answer:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa71254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Question: What is a binary tree and its applications? Answer: A binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child. Binary trees are used in various applications such as binary search trees (BSTs) for efficient searching, sorting, and insertion operations. They are also used in data compression algorithms like Huffman coding, and in expression trees for representing arithmetic expressions. Additionally, binary trees are used in graph algorithms for traversal and searching.\\n\\nAnswer: A binary tree is a tree data structure where each node has at most two children, referred to as the left and right child. They are used in various applications such as binary search trees (BSTs) for efficient searching, sorting, and insertion. Binary trees are also used in data compression algorithms like Huffman coding, and in expression trees for representing arithmetic expressions. Additionally, binary trees are used in graph algorithms for traversal and searching.\\n\\nTo create a binary tree in Python, you can use a class with left and right child attributes. Here's an example:\\n\\n```python\\nclass Node:\\n    def __init__(self, value):\\n        self.value = value\\n        self.left = None\\n        self.right = None\\n\\n# Creating a binary tree\\nroot = Node(1)\\nroot.left = Node\"]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8dcbe",
   "metadata": {},
   "source": [
    "# 2. How to compress models (bfloat16 -> dfloat11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6cfbb",
   "metadata": {},
   "source": [
    "## Official example. `Flux.1-dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4901bde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Downloading diffusers-0.36.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: dfloat11[cuda12] in /opt/conda/lib/python3.11/site-packages (0.5.0)\n",
      "Collecting importlib_metadata (from diffusers)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers) (3.18.0)\n",
      "Collecting httpx<1.0.0 (from diffusers)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in /opt/conda/lib/python3.11/site-packages (from diffusers) (0.36.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from diffusers) (2.2.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from diffusers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from diffusers) (0.7.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from diffusers) (11.0.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (from dfloat11[cuda12]) (1.12.0)\n",
      "Requirement already satisfied: dahuffman==0.4.2 in /opt/conda/lib/python3.11/site-packages (from dfloat11[cuda12]) (0.4.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (from dfloat11[cuda12]) (4.57.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from dfloat11[cuda12]) (4.67.1)\n",
      "Requirement already satisfied: cupy-cuda12x in /opt/conda/lib/python3.11/site-packages (from dfloat11[cuda12]) (13.6.0)\n",
      "Collecting anyio (from httpx<1.0.0->diffusers)\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->diffusers) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->diffusers)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->diffusers) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->diffusers)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.34.0->diffusers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->dfloat11[cuda12]) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate->dfloat11[cuda12]) (2.7.0+cu128)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /opt/conda/lib/python3.11/site-packages (from cupy-cuda12x->dfloat11[cuda12]) (0.8.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2.3.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from transformers->dfloat11[cuda12]) (0.22.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate->dfloat11[cuda12]) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate->dfloat11[cuda12]) (75.8.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate->dfloat11[cuda12]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate->dfloat11[cuda12]) (3.0.2)\n",
      "Downloading diffusers-0.36.0-py3-none-any.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: importlib_metadata, h11, anyio, httpcore, httpx, diffusers\n",
      "Successfully installed anyio-4.12.1 diffusers-0.36.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 importlib_metadata-8.7.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install requirements\n",
    "!pip install -U diffusers dfloat11[cuda12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/conda/lib/python3.11/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacf454de82d4f059f13166df42eeaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "!pip install ipywidgets\n",
    "\n",
    "login()     # Your huggingface access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2c651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaf78d1743d48459373ab608fedbd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb70e6c2b44e4f4da984fd153b0c8775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2359\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:119\u001b[39m, in \u001b[36mT5TokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mfrom_slow\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:108\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_slow \u001b[38;5;129;01mand\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    109\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot instantiate this tokenizer from a slow version. If it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms based on sentencepiece, make sure you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhave sentencepiece installed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    111\u001b[39m     )\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdfloat11\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compress_model\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load the FLUX.1 model in bfloat16 precision\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m pipe = \u001b[43mFluxPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblack-forest-labs/FLUX.1-dev\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model = pipe.transformer\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Compress the model via dfloat11 compression\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:1021\u001b[39m, in \u001b[36mDiffusionPipeline.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1015\u001b[39m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[32m   1016\u001b[39m     sub_model_dtype = (\n\u001b[32m   1017\u001b[39m         torch_dtype.get(name, torch_dtype.get(\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, torch.float32))\n\u001b[32m   1018\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1019\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m torch_dtype\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     loaded_sub_model = \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_model_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1046\u001b[39m     logger.info(\n\u001b[32m   1047\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1048\u001b[39m     )\n\u001b[32m   1050\u001b[39m init_kwargs[name] = loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/diffusers/pipelines/pipeline_loading_utils.py:876\u001b[39m, in \u001b[36mload_sub_model\u001b[39m\u001b[34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder, use_safetensors, dduf_entries, provider_options, quantization_config)\u001b[39m\n\u001b[32m    874\u001b[39m     loaded_sub_model = load_method(name, **loading_kwargs)\n\u001b[32m    875\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m os.path.isdir(os.path.join(cached_folder, name)):\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m     loaded_sub_model = \u001b[43mload_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mloading_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    878\u001b[39m     \u001b[38;5;66;03m# else load from the root directory\u001b[39;00m\n\u001b[32m    879\u001b[39m     loaded_sub_model = load_method(cached_folder, **loading_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2113\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2110\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2111\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2360\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2359\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2361\u001b[39m     logger.info(\n\u001b[32m   2362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2364\u001b[39m     )\n\u001b[32m   2365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:88\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "from dfloat11 import compress_model\n",
    "\n",
    "\n",
    "# Load the FLUX.1 model in bfloat16 precision\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "model = pipe.transformer\n",
    "\n",
    "# Compress the model via dfloat11 compression\n",
    "compress_model(\n",
    "    model=model,\n",
    "    pattern_dict={\n",
    "        \"transformer_blocks\\.\\d+\": (\n",
    "            \"norm1.linear\",\n",
    "            \"norm1_context.linear\",\n",
    "            \"attn.to_q\",\n",
    "            \"attn.to_k\",\n",
    "            \"attn.to_v\",\n",
    "            \"attn.add_k_proj\",\n",
    "            \"attn.add_v_proj\",\n",
    "            \"attn.add_q_proj\",\n",
    "            \"attn.to_out.0\",\n",
    "            \"attn.to_add_out\",\n",
    "            \"ff.net.0.proj\",\n",
    "            \"ff.net.2\",\n",
    "            \"ff_context.net.0.proj\",\n",
    "            \"ff_context.net.2\",\n",
    "        ),\n",
    "        \"single_transformer_blocks\\.\\d+\": (\n",
    "            \"norm.linear\",\n",
    "            \"proj_mlp\",\n",
    "            \"proj_out\",\n",
    "            \"attn.to_q\",\n",
    "            \"attn.to_k\",\n",
    "            \"attn.to_v\",\n",
    "        ),\n",
    "    },\n",
    "    save_path=\"./FLUX.1-dev-DF11\",\n",
    "    save_single_file=True,\n",
    "    check_correctness=True,\n",
    "    block_range=(0,100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference code. 20GB VRAM needed\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "from dfloat11 import DFloat11Model\n",
    "\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "DFloat11Model.from_pretrained('./FLUX.1-dev-DF11', device='cpu', bfloat16_model=pipe.transformer)   # \"./FLUX.1-dev-DF11\" 는 위에서 압축모델 저장한 path\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "prompt = \"A futuristic cityscape at sunset, with flying cars, neon lights, and reflective water canals\"\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(0)\n",
    ").images[0]\n",
    "\n",
    "image.save(\"image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699fd19",
   "metadata": {},
   "source": [
    "## Any other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAONE 4.0 1.2B\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-1.2B\", trust_remote_code=True)\n",
    "for name, module in model.named_modules():\n",
    "    print(name) # 이 출력을 보고 pattern_dict의 키(Key)와 값(Value)을 맞추면 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
